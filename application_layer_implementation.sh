#!/bin/bash

echo "ğŸš€ Applicationå±¤ï¼ˆãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…ï¼‰ã‚’å®Ÿè£…ä¸­..."

# ==================== 1. Scrapersï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å±¤ï¼‰å®Ÿè£… ====================
echo "ğŸ”¬ Scrapersï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å±¤ï¼‰ã‚’å®Ÿè£…ä¸­..."

# åŸºåº•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
cat > scraper/application/scrapers/university_scraper_base.py << 'EOF'
"""
å¤§å­¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹
å…±é€šã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’æä¾›
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Any
import asyncio
import logging
from urllib.parse import urljoin, urlparse

from scraper.config.interfaces import (
    UniversityScraperInterface, ResearchLabData, AdmissionData,
    ScrapingError, DataValidationError
)
from scraper.domain.university import University
from scraper.domain.research_lab import create_research_lab_from_data
from scraper.infrastructure.http.http_client import ResearchLabHttpClient
from scraper.infrastructure.parsers.content_parser import UniversityContentParser
from scraper.config.settings import scraping_settings

logger = logging.getLogger(__name__)


class UniversityScraperBase(UniversityScraperInterface):
    """å¤§å­¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, university: University):
        self.university = university
        self.base_url = university.info.website_url or ""
        self.http_client: Optional[ResearchLabHttpClient] = None
        self.parser: Optional[UniversityContentParser] = None
        self._scraped_labs: List[ResearchLabData] = []
        self._scraped_admissions: List[AdmissionData] = []
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        self.http_client = ResearchLabHttpClient()
        await self.http_client.__aenter__()
        self.parser = UniversityContentParser(self.university.name)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        if self.http_client:
            await self.http_client.__aexit__(exc_type, exc_val, exc_tb)
    
    async def scrape_research_labs(self) -> List[ResearchLabData]:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆåŸºæœ¬å®Ÿè£…ï¼‰"""
        try:
            logger.info(f"ğŸ” {self.university.name} ã®ç ”ç©¶å®¤ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            
            # å¯¾è±¡URLãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
            target_urls = await self._discover_target_urls()
            
            # å„URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            for url in target_urls:
                try:
                    labs = await self._scrape_single_page(url)
                    self._scraped_labs.extend(labs)
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                    logger.info(f"âœ… {url}: {len(labs)}ä»¶ã®ç ”ç©¶å®¤ã‚’ç™ºè¦‹")
                    
                except Exception as e:
                    logger.error(f"âŒ {url} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
                    continue
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            validated_labs = await self._validate_and_enrich_labs(self._scraped_labs)
            
            logger.info(f"ğŸ‰ {self.university.name}: åˆè¨ˆ{len(validated_labs)}ä»¶ã®ç ”ç©¶å®¤ã‚’åé›†")
            return validated_labs
            
        except Exception as e:
            logger.error(f"âŒ {self.university.name} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
            raise ScrapingError(f"Failed to scrape {self.university.name}: {e}")
    
    async def scrape_admission_info(self) -> List[AdmissionData]:
        """ç·åˆå‹é¸æŠœæƒ…å ±ã®å–å¾—ï¼ˆåŸºæœ¬å®Ÿè£…ï¼‰"""
        try:
            logger.info(f"ğŸ“‹ {self.university.name} ã®ç·åˆå‹é¸æŠœæƒ…å ±åé›†é–‹å§‹")
            
            # å…¥è©¦æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢
            admission_urls = await self._discover_admission_urls()
            
            for url in admission_urls:
                try:
                    admissions = await self._scrape_admission_page(url)
                    self._scraped_admissions.extend(admissions)
                    
                except Exception as e:
                    logger.error(f"âŒ ç·åˆå‹é¸æŠœæƒ…å ±åé›†å¤±æ•— {url}: {e}")
                    continue
            
            logger.info(f"ğŸ“Š {self.university.name}: {len(self._scraped_admissions)}ä»¶ã®é¸æŠœæƒ…å ±ã‚’åé›†")
            return self._scraped_admissions
            
        except Exception as e:
            logger.error(f"âŒ {self.university.name} ã®ç·åˆå‹é¸æŠœæƒ…å ±åé›†å¤±æ•—: {e}")
            return []
    
    def validate_data(self, data: ResearchLabData) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
            required_fields = ['name', 'professor_name', 'research_content']
            for field in required_fields:
                value = getattr(data, field, None)
                if not value or (isinstance(value, str) and len(value.strip()) < 3):
                    return False
            
            # ç ”ç©¶å†…å®¹ã®é•·ã•ãƒã‚§ãƒƒã‚¯
            if len(data.research_content) < scraping_settings.min_content_length:
                return False
            
            # å…ç–«é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œã§å®Ÿè£…ï¼‰
            # if data.immune_relevance_score and data.immune_relevance_score < 0.3:
            #     return False
            
            return True
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _discover_target_urls(self) -> List[str]:
        """å¯¾è±¡URLã‚’ç™ºè¦‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        urls = []
        
        if not self.base_url:
            logger.warning(f"{self.university.name}: ãƒ™ãƒ¼ã‚¹URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return urls
        
        # å­¦éƒ¨ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        faculty_urls = [
            f"{self.base_url}/medicine/",
            f"{self.base_url}/medical/",
            f"{self.base_url}/graduate/",
            f"{self.base_url}/research/",
            f"{self.base_url}/faculty/"
        ]
        
        urls.extend(faculty_urls)
        return urls
    
    async def _discover_admission_urls(self) -> List[str]:
        """ç·åˆå‹é¸æŠœURLã‚’ç™ºè¦‹"""
        urls = []
        
        if not self.base_url:
            return urls
        
        # å…¥è©¦æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        admission_urls = [
            f"{self.base_url}/admissions/",
            f"{self.base_url}/entrance/",
            f"{self.base_url}/exam/",
            f"{self.base_url}/nyushi/",
            f"{self.base_url}/ao/"
        ]
        
        urls.extend(admission_urls)
        return urls
    
    async def _scrape_single_page(self, url: str) -> List[ResearchLabData]:
        """å˜ä¸€ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            # HTMLã‚’å–å¾—
            html_content = await self.http_client.get_text(url)
            
            # ãƒ‘ãƒ¼ã‚¹
            self.parser.parse_html(html_content)
            
            # ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º
            raw_labs = self.parser.extract_research_labs()
            
            # ResearchLabDataã«å¤‰æ›
            lab_data_list = []
            for raw_lab in raw_labs:
                try:
                    lab_data = self._convert_to_research_lab_data(raw_lab, url)
                    if self.validate_data(lab_data):
                        lab_data_list.append(lab_data)
                except Exception as e:
                    logger.warning(f"ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿å¤‰æ›å¤±æ•—: {e}")
                    continue
            
            return lab_data_list
            
        except Exception as e:
            logger.error(f"ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— {url}: {e}")
            return []
    
    async def _scrape_admission_page(self, url: str) -> List[AdmissionData]:
        """ç·åˆå‹é¸æŠœãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            html_content = await self.http_client.get_text(url)
            # ç·åˆå‹é¸æŠœæƒ…å ±ã®æŠ½å‡ºï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…
            admission_data = AdmissionData(
                university_id=self.university.info.id,
                faculty="åŒ»å­¦éƒ¨",  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯å‹•çš„ã«æŠ½å‡º
                department=None,
                is_available=True,
                quota="è‹¥å¹²å",
                info_url=url
            )
            
            return [admission_data]
            
        except Exception as e:
            logger.error(f"ç·åˆå‹é¸æŠœãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— {url}: {e}")
            return []
    
    def _convert_to_research_lab_data(self, raw_lab: Dict[str, str], source_url: str) -> ResearchLabData:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ResearchLabDataã«å¤‰æ›"""
        return ResearchLabData(
            name=raw_lab.get('name', '').strip(),
            professor_name=raw_lab.get('professor_name', '').strip(),
            department=raw_lab.get('department', '').strip(),
            faculty=self._determine_faculty_from_url(source_url),
            research_content=raw_lab.get('research_content', '').strip(),
            university_id=self.university.info.id,
            lab_url=self._resolve_lab_url(raw_lab.get('lab_url', ''), source_url),
            keywords=raw_lab.get('keywords', ''),
            immune_relevance_score=raw_lab.get('immune_relevance_score')
        )
    
    def _determine_faculty_from_url(self, url: str) -> str:
        """URLã‹ã‚‰å­¦éƒ¨ã‚’æ¨å®š"""
        url_lower = url.lower()
        
        if any(keyword in url_lower for keyword in ['medical', 'medicine', 'åŒ»å­¦']):
            return 'medicine'
        elif any(keyword in url_lower for keyword in ['agriculture', 'è¾²å­¦', 'è¾²æ¥­']):
            return 'agriculture'
        elif any(keyword in url_lower for keyword in ['veterinary', 'ç£åŒ»']):
            return 'veterinary'
        elif any(keyword in url_lower for keyword in ['science', 'ç†å­¦']):
            return 'science'
        else:
            return 'general'
    
    def _resolve_lab_url(self, lab_url: str, base_url: str) -> str:
        """ç ”ç©¶å®¤URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
        if not lab_url:
            return ""
        
        if lab_url.startswith('http'):
            return lab_url
        
        return urljoin(base_url, lab_url)
    
    async def _validate_and_enrich_labs(self, labs: List[ResearchLabData]) -> List[ResearchLabData]:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å¼·åŒ–"""
        validated_labs = []
        
        for lab in labs:
            try:
                # åŸºæœ¬ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                if not self.validate_data(lab):
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿å¼·åŒ–ï¼ˆå…ç–«é–¢é€£åº¦è¨ˆç®—ãªã©ï¼‰
                enriched_lab = await self._enrich_lab_data(lab)
                validated_labs.append(enriched_lab)
                
            except Exception as e:
                logger.warning(f"ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å¤±æ•—: {e}")
                continue
        
        return validated_labs
    
    async def _enrich_lab_data(self, lab: ResearchLabData) -> ResearchLabData:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®å¼·åŒ–"""
        # å…ç–«é–¢é€£åº¦åˆ†æ
        from scraper.domain.keyword_analyzer import keyword_analyzer
        
        if lab.research_content:
            analysis_result = keyword_analyzer.analyze_content(lab.research_content)
            lab.immune_relevance_score = analysis_result.immune_relevance_score
            lab.research_field = analysis_result.field_classification
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±åˆ
            if analysis_result.matched_keywords:
                existing_keywords = lab.keywords or ''
                new_keywords = ', '.join(analysis_result.matched_keywords)
                lab.keywords = f"{existing_keywords}, {new_keywords}".strip(', ')
        
        return lab
EOF

# åŒ»å­¦éƒ¨å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
cat > scraper/application/scrapers/medical_scraper.py << 'EOF'
"""
åŒ»å­¦éƒ¨å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
åŒ»å­¦ç³»ç ”ç©¶å®¤ã«ç‰¹åŒ–ã—ãŸæƒ…å ±æŠ½å‡º
"""

import re
from typing import List, Dict
import logging

from scraper.application.scrapers.university_scraper_base import UniversityScraperBase
from scraper.domain.university import University

logger = logging.getLogger(__name__)


class MedicalLabScraper(UniversityScraperBase):
    """åŒ»å­¦éƒ¨ç ”ç©¶å®¤å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, university: University):
        super().__init__(university)
        self.medical_keywords = [
            'åŒ»å­¦éƒ¨', 'åŒ»å­¦ç ”ç©¶ç§‘', 'åŒ»å­¦é™¢', 'medical', 'medicine',
            'åŒ»ç™‚', 'è‡¨åºŠ', 'clinical', 'ç—…é™¢', 'hospital'
        ]
        self.immune_indicators = [
            'å…ç–«', 'immunology', 'immunity', 'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼', 'allergy',
            'ãŒã‚“å…ç–«', 'cancer immunology', 'æ„ŸæŸ“ç—‡', 'infection',
            'ãƒ¯ã‚¯ãƒãƒ³', 'vaccine', 'è‡ªå·±å…ç–«', 'autoimmune'
        ]
    
    async def _discover_target_urls(self) -> List[str]:
        """åŒ»å­¦éƒ¨ç‰¹åŒ–ã®URLç™ºè¦‹"""
        urls = []
        
        if not self.base_url:
            return urls
        
        # åŒ»å­¦éƒ¨ç‰¹åŒ–URL
        medical_urls = [
            f"{self.base_url}/medicine/",
            f"{self.base_url}/medical/",
            f"{self.base_url}/med/",
            f"{self.base_url}/graduate/medicine/",
            f"{self.base_url}/research/medicine/",
            f"{self.base_url}/faculty/medicine/",
            # æ—¥æœ¬èªURL
            f"{self.base_url}/åŒ»å­¦éƒ¨/",
            f"{self.base_url}/åŒ»å­¦ç ”ç©¶ç§‘/",
            f"{self.base_url}/ç ”ç©¶/åŒ»å­¦/"
        ]
        
        # å…ç–«å­¦ç‰¹åŒ–URL
        immune_urls = [
            f"{self.base_url}/immunology/",
            f"{self.base_url}/research/immunology/",
            f"{self.base_url}/å…ç–«å­¦/",
            f"{self.base_url}/ç ”ç©¶/å…ç–«/"
        ]
        
        urls.extend(medical_urls)
        urls.extend(immune_urls)
        
        # å¤§å­¦å›ºæœ‰ã®URLç™ºè¦‹
        university_specific_urls = await self._discover_university_specific_urls()
        urls.extend(university_specific_urls)
        
        return urls
    
    async def _discover_university_specific_urls(self) -> List[str]:
        """å¤§å­¦å›ºæœ‰ã®URLç™ºè¦‹"""
        urls = []
        university_name = self.university.name
        
        # å¤§å­¦åˆ¥ã®ç‰¹åˆ¥ãªURLæ§‹é€ 
        if "æ±äº¬å¤§å­¦" in university_name:
            urls.extend([
                f"{self.base_url}/faculty/medicine/",
                f"{self.base_url}/graduate/medicine/",
                "https://www.m.u-tokyo.ac.jp/research/"
            ])
        elif "äº¬éƒ½å¤§å­¦" in university_name:
            urls.extend([
                f"{self.base_url}/med/",
                "https://www.med.kyoto-u.ac.jp/research/"
            ])
        elif "å¤§é˜ªå¤§å­¦" in university_name:
            urls.extend([
                f"{self.base_url}/medicine/",
                "https://www.med.osaka-u.ac.jp/research/"
            ])
        elif "æ±åŒ—å¤§å­¦" in university_name:
            urls.extend([
                "https://www.med.tohoku.ac.jp/research/",
                "https://www.tohoku.ac.jp/japanese/research/"
            ])
        elif "åŒ—æµ·é“å¤§å­¦" in university_name:
            urls.extend([
                "https://www.med.hokudai.ac.jp/research/",
                "https://www.hokudai.ac.jp/research/"
            ])
        elif "åƒè‘‰å¤§å­¦" in university_name:
            urls.extend([
                "https://www.m.chiba-u.ac.jp/research/",
                "https://www.chiba-u.ac.jp/research/"
            ])
        elif "é †å¤©å ‚å¤§å­¦" in university_name:
            urls.extend([
                "https://www.juntendo.ac.jp/faculty/medicine/",
                "https://www.juntendo.ac.jp/research/"
            ])
        
        return urls
    
    async def _scrape_single_page(self, url: str) -> List[Dict[str, str]]:
        """åŒ»å­¦éƒ¨ç‰¹åŒ–ã®å˜ä¸€ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            # åŸºåº•ã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œ
            base_labs = await super()._scrape_single_page(url)
            
            # åŒ»å­¦éƒ¨ãƒ»å…ç–«å­¦ç‰¹åŒ–ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            medical_labs = []
            for lab in base_labs:
                if self._is_medical_immune_lab(lab):
                    enhanced_lab = self._enhance_medical_lab_data(lab)
                    medical_labs.append(enhanced_lab)
            
            logger.info(f"ğŸ¥ åŒ»å­¦éƒ¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(base_labs)}ä»¶ â†’ {len(medical_labs)}ä»¶")
            return medical_labs
            
        except Exception as e:
            logger.error(f"åŒ»å­¦éƒ¨ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— {url}: {e}")
            return []
    
    def _is_medical_immune_lab(self, lab_data) -> bool:
        """åŒ»å­¦ãƒ»å…ç–«é–¢é€£ç ”ç©¶å®¤ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        content_text = (
            lab_data.research_content + " " + 
            lab_data.name + " " + 
            lab_data.department
        ).lower()
        
        # åŒ»å­¦é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_medical = any(keyword.lower() in content_text for keyword in self.medical_keywords)
        
        # å…ç–«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_immune = any(keyword.lower() in content_text for keyword in self.immune_indicators)
        
        # åŒ»å­¦é–¢é€£ã¾ãŸã¯å…ç–«é–¢é€£ã§ã‚ã‚Œã°OK
        return has_medical or has_immune
    
    def _enhance_medical_lab_data(self, lab_data) -> Dict[str, str]:
        """åŒ»å­¦éƒ¨ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®å¼·åŒ–"""
        enhanced_data = lab_data.copy()
        
        # è¨ºç™‚ç§‘ãƒ»å°‚é–€åˆ†é‡ã®æŠ½å‡º
        specialties = self._extract_medical_specialties(lab_data.research_content)
        if specialties:
            existing_keywords = enhanced_data.get('keywords', '')
            specialty_keywords = ', '.join(specialties)
            enhanced_data['keywords'] = f"{existing_keywords}, {specialty_keywords}".strip(', ')
        
        # ç ”ç©¶æ‰‹æ³•ã®æŠ½å‡º
        methods = self._extract_research_methods(lab_data.research_content)
        if methods:
            enhanced_data['research_methods'] = ', '.join(methods)
        
        # å­¦éƒ¨ãƒ»ç ”ç©¶ç§‘ã®æ­£è¦åŒ–
        enhanced_data['faculty'] = 'medicine'
        
        return enhanced_data
    
    def _extract_medical_specialties(self, content: str) -> List[str]:
        """åŒ»å­¦å°‚é–€åˆ†é‡ã‚’æŠ½å‡º"""
        specialties = []
        content_lower = content.lower()
        
        medical_specialties = [
            'å†…ç§‘', 'internal medicine', 'å¤–ç§‘', 'surgery',
            'å°å…ç§‘', 'pediatrics', 'ç”£å©¦äººç§‘', 'obstetrics',
            'æ•´å½¢å¤–ç§‘', 'orthopedics', 'çš®è†šç§‘', 'dermatology',
            'çœ¼ç§‘', 'ophthalmology', 'è€³é¼»å’½å–‰ç§‘', 'otolaryngology',
            'ç²¾ç¥ç§‘', 'psychiatry', 'æ”¾å°„ç·šç§‘', 'radiology',
            'éº»é…”ç§‘', 'anesthesiology', 'ç—…ç†', 'pathology',
            'å…ç–«å­¦', 'immunology', 'æ„ŸæŸ“ç—‡å­¦', 'infectious disease',
            'ãŒã‚“å­¦', 'oncology', 'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼å­¦', 'allergology'
        ]
        
        for specialty in medical_specialties:
            if specialty.lower() in content_lower:
                specialties.append(specialty)
        
        return list(set(specialties))
    
    def _extract_research_methods(self, content: str) -> List[str]:
        """ç ”ç©¶æ‰‹æ³•ã‚’æŠ½å‡º"""
        methods = []
        content_lower = content.lower()
        
        research_methods = [
            'PCR', 'qPCR', 'RT-PCR', 'Western blot', 'ELISA',
            'FACS', 'ãƒ•ãƒ­ãƒ¼ã‚µã‚¤ãƒˆãƒ¡ãƒˆãƒªãƒ¼', 'flow cytometry',
            'RNA-seq', 'ChIP-seq', 'ãƒã‚¤ã‚¯ãƒ­ã‚¢ãƒ¬ã‚¤', 'microarray',
            'CRISPR', 'ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°', 'cloning',
            'ç´°èƒåŸ¹é¤Š', 'cell culture', 'in vitro', 'in vivo',
            'å‹•ç‰©å®Ÿé¨“', 'animal model', 'ãƒã‚¦ã‚¹', 'mouse',
            'è‡¨åºŠè©¦é¨“', 'clinical trial', 'ç–«å­¦èª¿æŸ»', 'epidemiology'
        ]
        
        for method in research_methods:
            if method.lower() in content_lower:
                methods.append(method)
        
        return list(set(methods))
EOF

# è¾²å­¦éƒ¨å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆæ–°è¦ï¼‰
cat > scraper/application/scrapers/agriculture_scraper.py << 'EOF'
"""
è¾²å­¦éƒ¨å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆæ–°è¦å®Ÿè£…ï¼‰
è¾²å­¦ãƒ»é£Ÿå“ãƒ»å‹•ç‰©å…ç–«ç ”ç©¶ã«ç‰¹åŒ–
"""

import re
from typing import List, Dict
import logging

from scraper.application.scrapers.university_scraper_base import UniversityScraperBase
from scraper.domain.university import University

logger = logging.getLogger(__name__)


class AgricultureLabScraper(UniversityScraperBase):
    """è¾²å­¦éƒ¨ç ”ç©¶å®¤å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, university: University):
        super().__init__(university)
        self.agriculture_keywords = [
            'è¾²å­¦éƒ¨', 'è¾²å­¦ç ”ç©¶ç§‘', 'è¾²æ¥­', 'agriculture', 'agricultural',
            'é£Ÿå“', 'food', 'æ „é¤Š', 'nutrition', 'ç”Ÿç‰©è³‡æº', 'bioresource'
        ]
        self.animal_immune_indicators = [
            'å‹•ç‰©å…ç–«', 'animal immunity', 'å®¶ç•œå…ç–«', 'livestock immunity',
            'ç£åŒ»', 'veterinary', 'å‹•ç‰©', 'animal', 'å®¶ç•œ', 'livestock',
            'ç‰›', 'cattle', 'è±š', 'pig', 'é¶', 'chicken', 'é­š', 'fish'
        ]
        self.plant_indicators = [
            'æ¤ç‰©å…ç–«', 'plant immunity', 'æ¤ç‰©ç—…ç†', 'plant pathology',
            'ä½œç‰©', 'crop', 'æ¤ç‰©', 'plant', 'è‚²ç¨®', 'breeding'
        ]
        self.food_indicators = [
            'é£Ÿå“å…ç–«', 'food immunity', 'æ©Ÿèƒ½æ€§é£Ÿå“', 'functional food',
            'ãƒ—ãƒ­ãƒã‚¤ã‚ªãƒ†ã‚£ã‚¯ã‚¹', 'probiotics', 'ç™ºé…µ', 'fermentation',
            'è…¸å†…ç´°èŒ', 'gut microbiota'
        ]
    
    async def _discover_target_urls(self) -> List[str]:
        """è¾²å­¦éƒ¨ç‰¹åŒ–ã®URLç™ºè¦‹"""
        urls = []
        
        if not self.base_url:
            return urls
        
        # è¾²å­¦éƒ¨ç‰¹åŒ–URL
        agriculture_urls = [
            f"{self.base_url}/agriculture/",
            f"{self.base_url}/agricultural/",
            f"{self.base_url}/agr/",
            f"{self.base_url}/bioresource/",
            f"{self.base_url}/food/",
            f"{self.base_url}/veterinary/",
            # æ—¥æœ¬èªURL
            f"{self.base_url}/è¾²å­¦éƒ¨/",
            f"{self.base_url}/è¾²å­¦ç ”ç©¶ç§‘/",
            f"{self.base_url}/ç£åŒ»å­¦éƒ¨/",
            f"{self.base_url}/ç”Ÿç‰©è³‡æº/"
        ]
        
        # å‹•ç‰©ãƒ»é£Ÿå“ãƒ»æ¤ç‰©å…ç–«ç‰¹åŒ–URL
        specialized_urls = [
            f"{self.base_url}/animal/",
            f"{self.base_url}/livestock/",
            f"{self.base_url}/veterinary/",
            f"{self.base_url}/plant/",
            f"{self.base_url}/crop/",
            f"{self.base_url}/food/",
            f"{self.base_url}/nutrition/"
        ]
        
        urls.extend(agriculture_urls)
        urls.extend(specialized_urls)
        
        # è¾²å­¦ç³»å¤§å­¦å›ºæœ‰URL
        university_specific_urls = await self._discover_agriculture_specific_urls()
        urls.extend(university_specific_urls)
        
        return urls
    
    async def _discover_agriculture_specific_urls(self) -> List[str]:
        """è¾²å­¦ç³»å¤§å­¦å›ºæœ‰ã®URLç™ºè¦‹"""
        urls = []
        university_name = self.university.name
        
        # è¾²å­¦ç³»å¤§å­¦åˆ¥ã®ç‰¹åˆ¥ãªURLæ§‹é€ 
        if "æ±äº¬è¾²å·¥å¤§å­¦" in university_name:
            urls.extend([
                "https://www.tuat.ac.jp/outline/faculty/agriculture/",
                "https://www.tuat.ac.jp/research/"
            ])
        elif "åŒ—æµ·é“å¤§å­¦" in university_name:
            urls.extend([
                "https://www.agr.hokudai.ac.jp/",
                "https://www.vet.hokudai.ac.jp/"
            ])
        elif "å¸¯åºƒç•œç”£å¤§å­¦" in university_name:
            urls.extend([
                "https://www.obihiro.ac.jp/research/",
                "https://www.obihiro.ac.jp/faculty/"
            ])
        elif "å²é˜œå¤§å­¦" in university_name:
            urls.extend([
                "https://www.gifu-u.ac.jp/education/faculty/applied_biological_sciences/",
                "https://www.gifu-u.ac.jp/research/"
            ])
        elif "é³¥å–å¤§å­¦" in university_name:
            urls.extend([
                "https://www.tottori-u.ac.jp/dd.aspx?menuid=1414",
                "https://www.ag.tottori-u.ac.jp/"
            ])
        elif "å®®å´å¤§å­¦" in university_name:
            urls.extend([
                "https://www.miyazaki-u.ac.jp/agriculture/",
                "https://www.vet.miyazaki-u.ac.jp/"
            ])
        
        return urls
    
    def _is_agriculture_immune_lab(self, lab_data) -> bool:
        """è¾²å­¦ãƒ»å‹•ç‰©ãƒ»é£Ÿå“å…ç–«é–¢é€£ç ”ç©¶å®¤ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        content_text = (
            lab_data.research_content + " " + 
            lab_data.name + " " + 
            lab_data.department
        ).lower()
        
        # è¾²å­¦é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_agriculture = any(keyword.lower() in content_text for keyword in self.agriculture_keywords)
        
        # å‹•ç‰©å…ç–«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_animal_immune = any(keyword.lower() in content_text for keyword in self.animal_immune_indicators)
        
        # æ¤ç‰©å…ç–«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_plant_immune = any(keyword.lower() in content_text for keyword in self.plant_indicators)
        
        # é£Ÿå“å…ç–«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        has_food_immune = any(keyword.lower() in content_text for keyword in self.food_indicators)
        
        # ã„ãšã‚Œã‹ã®è¾²å­¦ãƒ»å…ç–«é–¢é€£ã§ã‚ã‚Œã°OK
        return has_agriculture or has_animal_immune or has_plant_immune or has_food_immune
    
    def _enhance_agriculture_lab_data(self, lab_data) -> Dict[str, str]:
        """è¾²å­¦éƒ¨ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®å¼·åŒ–"""
        enhanced_data = lab_data.copy()
        
        # å‹•ç‰©ç¨®ã®æŠ½å‡º
        animal_species = self._extract_animal_species(lab_data.research_content)
        if animal_species:
            enhanced_data['animal_species'] = ', '.join(animal_species)
        
        # æ¤ç‰©ç¨®ã®æŠ½å‡º
        plant_species = self._extract_plant_species(lab_data.research_content)
        if plant_species:
            enhanced_data['plant_species'] = ', '.join(plant_species)
        
        # è¾²å­¦ç ”ç©¶åˆ†é‡ã®ç‰¹å®š
        research_area = self._determine_agriculture_research_area(lab_data.research_content)
        enhanced_data['agriculture_research_area'] = research_area
        
        # å­¦éƒ¨ãƒ»ç ”ç©¶ç§‘ã®æ­£è¦åŒ–
        enhanced_data['faculty'] = 'agriculture'
        
        return enhanced_data
    
    def _extract_animal_species(self, content: str) -> List[str]:
        """å‹•ç‰©ç¨®ã‚’æŠ½å‡º"""
        species = []
        content_lower = content.lower()
        
        animal_species_list = [
            'ç‰›', 'cattle', 'bovine', 'ä¹³ç‰›', 'dairy cow',
            'è±š', 'pig', 'swine', 'porcine',
            'é¶', 'chicken', 'poultry', 'å®¶ç¦½',
            'é­š', 'fish', 'é¤Šæ®–é­š', 'aquaculture',
            'ç¾Š', 'sheep', 'ovine', 'å±±ç¾Š', 'goat',
            'é¦¬', 'horse', 'equine', 'çŠ¬', 'dog', 'canine',
            'çŒ«', 'cat', 'feline', 'ãƒã‚¦ã‚¹', 'mouse',
            'ãƒ©ãƒƒãƒˆ', 'rat', 'ã‚¦ã‚µã‚®', 'rabbit'
        ]
        
        for animal in animal_species_list:
            if animal.lower() in content_lower:
                species.append(animal)
        
        return list(set(species))
    
    def _extract_plant_species(self, content: str) -> List[str]:
        """æ¤ç‰©ç¨®ã‚’æŠ½å‡º"""
        species = []
        content_lower = content.lower()
        
        plant_species_list = [
            'ç¨²', 'rice', 'ã‚¤ãƒ', 'ãƒˆãƒãƒˆ', 'tomato',
            'å¤§è±†', 'soybean', 'å°éº¦', 'wheat',
            'ãƒˆã‚¦ãƒ¢ãƒ­ã‚³ã‚·', 'corn', 'maize', 'ã‚¸ãƒ£ã‚¬ã‚¤ãƒ¢', 'potato',
            'ã‚­ãƒ£ãƒ™ãƒ„', 'cabbage', 'ãƒ‹ãƒ³ã‚¸ãƒ³', 'carrot',
            'ãƒªãƒ³ã‚´', 'apple', 'ãƒŸã‚«ãƒ³', 'orange',
            'ãƒãƒ©', 'rose', 'ã‚¢ãƒ©ãƒ“ãƒ‰ãƒ—ã‚·ã‚¹', 'arabidopsis'
        ]
        
        for plant in plant_species_list:
            if plant.lower() in content_lower:
                species.append(plant)
        
        return list(set(species))
    
    def _determine_agriculture_research_area(self, content: str) -> str:
        """è¾²å­¦ç ”ç©¶åˆ†é‡ã‚’ç‰¹å®š"""
        content_lower = content.lower()
        
        if any(kw in content_lower for kw in ['animal', 'å‹•ç‰©', 'livestock', 'å®¶ç•œ']):
            return 'å‹•ç‰©ç§‘å­¦'
        elif any(kw in content_lower for kw in ['plant', 'æ¤ç‰©', 'crop', 'ä½œç‰©']):
            return 'æ¤ç‰©ç§‘å­¦'
        elif any(kw in content_lower for kw in ['food', 'é£Ÿå“', 'nutrition', 'æ „é¤Š']):
            return 'é£Ÿå“ç§‘å­¦'
        elif any(kw in content_lower for kw in ['environment', 'ç’°å¢ƒ', 'ecology', 'ç”Ÿæ…‹']):
            return 'ç’°å¢ƒç§‘å­¦'
        elif any(kw in content_lower for kw in ['biotechnology', 'ãƒã‚¤ã‚ªãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'genetic']):
            return 'ãƒã‚¤ã‚ªãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        else:
            return 'è¾²å­¦ä¸€èˆ¬'
EOF

echo "âœ… Scrapersï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼å±¤ï¼‰å®Ÿè£…å®Œäº†"

# ==================== 2. Pipelinesï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰å®Ÿè£… ====================
echo "âš™ï¸ Pipelinesï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰ã‚’å®Ÿè£…ä¸­..."

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
cat > scraper/application/pipelines/lab_processing.py << 'EOF'
"""
ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆãƒ»å¼·åŒ–ãƒ»å“è³ªç®¡ç†
"""

import asyncio
from typing import List, Dict, Optional, Tuple
import logging
from datetime import datetime

from scraper.config.interfaces import ResearchLabData, DataValidationError
from scraper.domain.research_lab import ResearchLab, create_research_lab_from_data
from scraper.domain.university import University
from scraper.domain.keyword_analyzer import keyword_analyzer
from scraper.config.settings import quality_settings

logger = logging.getLogger(__name__)


class LabDataProcessor:
    """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self):
        self.processed_count = 0
        self.rejected_count = 0
        self.duplicate_count = 0
        self.enhanced_count = 0
    
    async def process_lab_data_batch(
        self, 
        raw_labs: List[ResearchLabData],
        university: University
    ) -> List[ResearchLab]:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒå‡¦ç†"""
        logger.info(f"ğŸ”„ {university.name}: {len(raw_labs)}ä»¶ã®ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
        
        # Step 1: åŸºæœ¬ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        validated_labs = await self._validate_basic_requirements(raw_labs)
        logger.info(f"ğŸ“‹ åŸºæœ¬ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: {len(validated_labs)}/{len(raw_labs)}ä»¶åˆæ ¼")
        
        # Step 2: é‡è¤‡é™¤å»
        deduplicated_labs = await self._remove_duplicates(validated_labs)
        self.duplicate_count += len(validated_labs) - len(deduplicated_labs)
        logger.info(f"ğŸ” é‡è¤‡é™¤å»: {len(deduplicated_labs)}ä»¶ï¼ˆ{self.duplicate_count}ä»¶é™¤å»ï¼‰")
        
        # Step 3: ãƒ‡ãƒ¼ã‚¿å¼·åŒ–
        enhanced_labs = await self._enhance_lab_data(deduplicated_labs, university)
        logger.info(f"âš¡ ãƒ‡ãƒ¼ã‚¿å¼·åŒ–: {len(enhanced_labs)}ä»¶")
        
        # Step 4: å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        quality_filtered_labs = await self._filter_by_quality(enhanced_labs)
        self.rejected_count += len(enhanced_labs) - len(quality_filtered_labs)
        logger.info(f"ğŸ¯ å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(quality_filtered_labs)}ä»¶ï¼ˆ{len(enhanced_labs) - len(quality_filtered_labs)}ä»¶é™¤å¤–ï¼‰")
        
        # Step 5: ResearchLabã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä½œæˆ
        research_labs = await self._create_research_lab_entities(quality_filtered_labs, university)
        self.processed_count += len(research_labs)
        
        logger.info(f"âœ… {university.name}: {len(research_labs)}ä»¶ã®ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†")
        return research_labs
    
    async def _validate_basic_requirements(self, labs: List[ResearchLabData]) -> List[ResearchLabData]:
        """åŸºæœ¬è¦ä»¶ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        validated_labs = []
        
        for lab in labs:
            try:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
                if not lab.name or len(lab.name.strip()) < 3:
                    continue
                
                if not lab.professor_name or len(lab.professor_name.strip()) < 2:
                    continue
                
                if not lab.research_content or len(lab.research_content) < quality_settings.min_content_length:
                    continue
                
                # ç ”ç©¶å†…å®¹ã®è³ªãƒã‚§ãƒƒã‚¯
                if not self._is_meaningful_content(lab.research_content):
                    continue
                
                validated_labs.append(lab)
                
            except Exception as e:
                logger.warning(f"åŸºæœ¬ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—: {e}")
                continue
        
        return validated_labs
    
    def _is_meaningful_content(self, content: str) -> bool:
        """ç ”ç©¶å†…å®¹ãŒæ„å‘³ã®ã‚ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ãƒã‚§ãƒƒã‚¯"""
        if not content:
            return False
        
        # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
        if len(content) < 50:
            return False
        
        # æ„å‘³ã®ã‚ã‚‹å˜èªã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        meaningful_words = [
            'ç ”ç©¶', 'research', 'é–‹ç™º', 'development', 'è§£æ', 'analysis',
            'å®Ÿé¨“', 'experiment', 'æ¤œè¨', 'èª¿æŸ»', 'investigation',
            'æŠ€è¡“', 'technology', 'æ‰‹æ³•', 'method', 'ç†è«–', 'theory'
        ]
        
        word_count = sum(1 for word in meaningful_words if word.lower() in content.lower())
        return word_count >= 2
    
    async def _remove_duplicates(self, labs: List[ResearchLabData]) -> List[ResearchLabData]:
        """é‡è¤‡ç ”ç©¶å®¤ã®é™¤å»"""
        seen_signatures = set()
        unique_labs = []
        
        for lab in labs:
            # é‡è¤‡åˆ¤å®šã®ã‚·ã‚°ãƒãƒãƒ£ä½œæˆ
            signature = self._create_lab_signature(lab)
            
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_labs.append(lab)
        
        return unique_labs
    
    def _create_lab_signature(self, lab: ResearchLabData) -> str:
        """ç ”ç©¶å®¤ã®é‡è¤‡åˆ¤å®šç”¨ã‚·ã‚°ãƒãƒãƒ£ä½œæˆ"""
        # ç ”ç©¶å®¤åã¨æ•™æˆåã‚’æ­£è¦åŒ–ã—ã¦çµåˆ
        normalized_name = self._normalize_text(lab.name)
        normalized_professor = self._normalize_text(lab.professor_name)
        
        return f"{normalized_name}#{normalized_professor}#{lab.university_id}"
    
    def _normalize_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–"""
        if not text:
            return ""
        
        # å°æ–‡å­—åŒ–ã€ç©ºç™½é™¤å»ã€è¨˜å·é™¤å»
        import re
        normalized = re.sub(r'[^\w\s]', '', text.lower())
        normalized = re.sub(r'\s+', '', normalized)
        
        return normalized
    
    async def _enhance_lab_data(self, labs: List[ResearchLabData], university: University) -> List[ResearchLabData]:
        """ãƒ‡ãƒ¼ã‚¿å¼·åŒ–å‡¦ç†"""
        enhanced_labs = []
        
        for lab in labs:
            try:
                enhanced_lab = await self._enhance_single_lab(lab, university)
                enhanced_labs.append(enhanced_lab)
                self.enhanced_count += 1
                
            except Exception as e:
                logger.warning(f"ãƒ‡ãƒ¼ã‚¿å¼·åŒ–å¤±æ•— {lab.name}: {e}")
                # å¼·åŒ–ã«å¤±æ•—ã—ã¦ã‚‚ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒ
                enhanced_labs.append(lab)
        
        return enhanced_labs
    
    async def _enhance_single_lab(self, lab: ResearchLabData, university: University) -> ResearchLabData:
        """å˜ä¸€ç ”ç©¶å®¤ã®ãƒ‡ãƒ¼ã‚¿å¼·åŒ–"""
        enhanced_lab = lab
        
        # å…ç–«é–¢é€£åº¦åˆ†æ
        if lab.research_content:
            analysis_result = keyword_analyzer.analyze_content(lab.research_content)
            
            enhanced_lab.immune_relevance_score = analysis_result.immune_relevance_score
            enhanced_lab.research_field = analysis_result.field_classification
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±åˆ
            if analysis_result.matched_keywords:
                existing_keywords = lab.keywords or ''
                new_keywords = ', '.join(analysis_result.matched_keywords)
                enhanced_lab.keywords = f"{existing_keywords}, {new_keywords}".strip(', ')
            
            # å‹•ç‰©ç¨®ãƒ»æ¤ç‰©ç¨®æƒ…å ±
            if analysis_result.animal_species:
                enhanced_lab.animal_species = ', '.join(analysis_result.animal_species)
            
            if analysis_result.plant_species:
                enhanced_lab.plant_species = ', '.join(analysis_result.plant_species)
        
        # å¤§å­¦æƒ…å ±ã®çµ±åˆ
        enhanced_lab.university_id = university.info.id
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
        if not enhanced_lab.metadata:
            enhanced_lab.metadata = {}
        
        enhanced_lab.metadata.update({
            'processed_at': datetime.now().isoformat(),
            'university_tier': 1 if university.is_tier1_university() else 2 if university.is_tier2_university() else 3,
            'university_type': university.type.value,
            'university_region': university.region
        })
        
        return enhanced_lab
    
    async def _filter_by_quality(self, labs: List[ResearchLabData]) -> List[ResearchLabData]:
        """å“è³ªåŸºæº–ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        quality_labs = []
        
        for lab in labs:
            try:
                quality_score = self._calculate_quality_score(lab)
                
                # å“è³ªã‚¹ã‚³ã‚¢ãŒåŸºæº–ã‚’æº€ãŸã™å ´åˆã®ã¿é€šã™
                if quality_score >= 0.6:  # 60%ä»¥ä¸Šã®å“è³ªã‚¹ã‚³ã‚¢
                    if not lab.metadata:
                        lab.metadata = {}
                    lab.metadata['quality_score'] = quality_score
                    quality_labs.append(lab)
                
            except Exception as e:
                logger.warning(f"å“è³ªè©•ä¾¡å¤±æ•— {lab.name}: {e}")
                continue
        
        return quality_labs
    
    def _calculate_quality_score(self, lab: ResearchLabData) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0.0
        total_weight = 0.0
        
        # åŸºæœ¬æƒ…å ±ã®å®Œå…¨æ€§ï¼ˆ50%ï¼‰
        if lab.name and len(lab.name) > 5:
            score += 0.2; total_weight += 0.2
        if lab.professor_name and len(lab.professor_name) > 2:
            score += 0.15; total_weight += 0.15
        if lab.research_content and len(lab.research_content) > 100:
            score += 0.15; total_weight += 0.15
        
        # è©³ç´°æƒ…å ±ã®å……å®Ÿåº¦ï¼ˆ30%ï¼‰
        if lab.keywords and len(lab.keywords.split(',')) >= 3:
            score += 0.15; total_weight += 0.15
        if lab.lab_url:
            score += 0.15; total_weight += 0.15
        
        # å…ç–«é–¢é€£åº¦ï¼ˆ20%ï¼‰
        if lab.immune_relevance_score and lab.immune_relevance_score >= quality_settings.min_immune_relevance_score:
            score += 0.2; total_weight += 0.2
        
        return score / total_weight if total_weight > 0 else 0.0
    
    async def _create_research_lab_entities(self, labs: List[ResearchLabData], university: University) -> List[ResearchLab]:
        """ResearchLabã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä½œæˆ"""
        research_labs = []
        
        for lab_data in labs:
            try:
                research_lab = create_research_lab_from_data(
                    data=lab_data,
                    university_info=university.info
                )
                research_labs.append(research_lab)
                
            except Exception as e:
                logger.error(f"ResearchLabã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä½œæˆå¤±æ•— {lab_data.name}: {e}")
                continue
        
        return research_labs
    
    def get_processing_stats(self) -> Dict[str, int]:
        """å‡¦ç†çµ±è¨ˆã‚’å–å¾—"""
        return {
            'processed_count': self.processed_count,
            'rejected_count': self.rejected_count,
            'duplicate_count': self.duplicate_count,
            'enhanced_count': self.enhanced_count
        }
EOF

echo "âœ… Pipelinesï¼ˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰å®Ÿè£…å®Œäº†"

# ==================== 3. Orchestrationï¼ˆãƒ—ãƒ­ã‚»ã‚¹èª¿æ•´ï¼‰å®Ÿè£… ====================
echo "ğŸ¯ Orchestrationï¼ˆãƒ—ãƒ­ã‚»ã‚¹èª¿æ•´ï¼‰ã‚’å®Ÿè£…ä¸­..."

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒª
cat > scraper/application/orchestration/scraper_factory.py << 'EOF'
"""
ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒª
å¤§å­¦ãƒ»å­¦éƒ¨ã«å¿œã˜ãŸé©åˆ‡ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ç”Ÿæˆ
"""

from typing import Dict, Type, List, Optional
import logging

from scraper.domain.university import University
from scraper.application.scrapers.university_scraper_base import UniversityScraperBase
from scraper.application.scrapers.medical_scraper import MedicalLabScraper
from scraper.application.scrapers.agriculture_scraper import AgricultureLabScraper
from scraper.config.interfaces import ScrapingError

logger = logging.getLogger(__name__)


class ScraperFactory:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒª"""
    
    def __init__(self):
        self._scraper_registry: Dict[str, Type[UniversityScraperBase]] = {
            'medical': MedicalLabScraper,
            'agriculture': AgricultureLabScraper,
            'veterinary': AgricultureLabScraper,  # è¾²å­¦ç³»ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’æµç”¨
            'general': UniversityScraperBase
        }
    
    def create_scraper(self, university: University, faculty_type: str = None) -> UniversityScraperBase:
        """
        å¤§å­¦ãƒ»å­¦éƒ¨ã«å¿œã˜ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½œæˆ
        
        Args:
            university: å¤§å­¦ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
            faculty_type: å­¦éƒ¨ç¨®åˆ¥ï¼ˆè‡ªå‹•åˆ¤å®šã‚‚å¯èƒ½ï¼‰
        
        Returns:
            UniversityScraperBase: é©åˆ‡ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
        """
        try:
            # å­¦éƒ¨ç¨®åˆ¥ã®è‡ªå‹•åˆ¤å®š
            if not faculty_type:
                faculty_type = self._determine_faculty_type(university)
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã®é¸æŠ
            scraper_class = self._scraper_registry.get(faculty_type, UniversityScraperBase)
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆ
            scraper = scraper_class(university)
            
            logger.info(f"âœ… {university.name} ç”¨ {scraper_class.__name__} ã‚’ä½œæˆ")
            return scraper
            
        except Exception as e:
            logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆå¤±æ•— {university.name}: {e}")
            raise ScrapingError(f"Failed to create scraper for {university.name}: {e}")
    
    def create_multiple_scrapers(self, universities: List[University]) -> List[UniversityScraperBase]:
        """è¤‡æ•°å¤§å­¦ç”¨ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä¸€æ‹¬ä½œæˆ"""
        scrapers = []
        
        for university in universities:
            try:
                scraper = self.create_scraper(university)
                scrapers.append(scraper)
            except Exception as e:
                logger.error(f"âŒ {university.name} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                continue
        
        logger.info(f"ğŸ­ {len(scrapers)}/{len(universities)} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ä½œæˆ")
        return scrapers
    
    def _determine_faculty_type(self, university: University) -> str:
        """å¤§å­¦ã®ç‰¹å¾´ã‹ã‚‰å­¦éƒ¨ç¨®åˆ¥ã‚’è‡ªå‹•åˆ¤å®š"""
        university_name = university.name.lower()
        
        # è¾²å­¦ãƒ»ç£åŒ»å­¦ç³»å¤§å­¦
        agriculture_indicators = [
            'è¾²å·¥å¤§', 'è¾²æ¥­å¤§', 'ç•œç”£å¤§', 'ç£åŒ»å¤§',
            'agriculture', 'veterinary', 'livestock'
        ]
        
        if any(indicator in university_name for indicator in agriculture_indicators):
            return 'agriculture'
        
        # åŒ»ç§‘å¤§å­¦
        medical_indicators = [
            'åŒ»ç§‘å¤§', 'åŒ»å¤§', 'åŒ»ç™‚å¤§',
            'medical', 'medicine'
        ]
        
        if any(indicator in medical_indicators for indicator in medical_indicators):
            return 'medical'
        
        # å­¦éƒ¨æƒ…å ±ã‹ã‚‰åˆ¤å®š
        if university.has_medical_faculty() and university.has_agriculture_faculty():
            # ä¸¡æ–¹ã‚ã‚‹å ´åˆã¯ã€å„ªå…ˆåº¦ã§æ±ºå®šï¼ˆå…ç–«ç ”ç©¶ã®è¦³ç‚¹ï¼‰
            return 'medical'
        elif university.has_medical_faculty():
            return 'medical'
        elif university.has_agriculture_faculty():
            return 'agriculture'
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åŒ»å­¦ç³»ï¼ˆå…ç–«ç ”ç©¶ã®ä¸»è¦åˆ†é‡ï¼‰
        return 'medical'
    
    def register_scraper(self, faculty_type: str, scraper_class: Type[UniversityScraperBase]) -> None:
        """æ–°ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ç™»éŒ²"""
        self._scraper_registry[faculty_type] = scraper_class
        logger.info(f"ğŸ“ æ–°ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’ç™»éŒ²: {faculty_type} -> {scraper_class.__name__}")
    
    def get_available_faculty_types(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªå­¦éƒ¨ç¨®åˆ¥ã‚’å–å¾—"""
        return list(self._scraper_registry.keys())
    
    def get_scraper_info(self) -> Dict[str, str]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æƒ…å ±ã‚’å–å¾—"""
        return {
            faculty_type: scraper_class.__name__ 
            for faculty_type, scraper_class in self._scraper_registry.items()
        }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
scraper_factory = ScraperFactory()
EOF

echo "âœ… Orchestrationï¼ˆãƒ—ãƒ­ã‚»ã‚¹èª¿æ•´ï¼‰å®Ÿè£…å®Œäº†"

echo ""
echo "ğŸ‰ Applicationå±¤å®Ÿè£…å®Œäº†ï¼"
echo ""
echo "ğŸ“‹ å®Ÿè£…ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:"
echo "â”œâ”€â”€ scraper/application/"
echo "â”‚   â”œâ”€â”€ scrapers/"
echo "â”‚   â”‚   â”œâ”€â”€ university_scraper_base.py   # åŸºåº•ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"
echo "â”‚   â”‚   â”œâ”€â”€ medical_scraper.py           # åŒ»å­¦éƒ¨å°‚ç”¨ï¼ˆæ—¢å­˜å¼·åŒ–ï¼‰"
echo "â”‚   â”‚   â””â”€â”€ agriculture_scraper.py       # è¾²å­¦éƒ¨å°‚ç”¨ï¼ˆæ–°è¦ï¼‰"
echo "â”‚   â”œâ”€â”€ pipelines/"
echo "â”‚   â”‚   â””â”€â”€ lab_processing.py            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
echo "â”‚   â””â”€â”€ orchestration/"
echo "â”‚       â””â”€â”€ scraper_factory.py           # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒª"
echo ""
echo "ğŸš€ Applicationå±¤ã®ç‰¹å¾´:"
echo "â€¢ å­¦éƒ¨ç‰¹åŒ–ï¼šåŒ»å­¦éƒ¨ãƒ»è¾²å­¦éƒ¨ãƒ»ç£åŒ»å­¦éƒ¨å¯¾å¿œ"
echo "â€¢ ãƒ‡ãƒ¼ã‚¿å“è³ªï¼šè‡ªå‹•ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»å¼·åŒ–ãƒ»é‡è¤‡é™¤å»"
echo "â€¢ æ‹¡å¼µæ€§ï¼šãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹æ–°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼è¿½åŠ "
echo "â€¢ å…ç–«ç‰¹åŒ–ï¼šå…ç–«é–¢é€£åº¦è‡ªå‹•åˆ¤å®šãƒ»åˆ†é‡åˆ†é¡"
echo "â€¢ è¾²å­¦å¯¾å¿œï¼šå‹•ç‰©ç¨®ãƒ»æ¤ç‰©ç¨®ã®è‡ªå‹•æŠ½å‡º"
echo ""
echo "âš¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š"
echo "1. çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"
echo "2. å®Ÿéš›ã®å¤§å­¦ã‚µã‚¤ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ"
echo "3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é€£æºã®ãƒ†ã‚¹ãƒˆ"