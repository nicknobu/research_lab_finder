# å…ç–«ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ‹¡å……æˆ¦ç•¥ï¼ˆå®Œå…¨ç‰ˆï¼‰ğŸ”¬

## ğŸ¯ ç›®æ¨™
- **å¯¾è±¡**: æ—¥æœ¬ã®å›½å…¬ç«‹ãƒ»ç§ç«‹å¤§å­¦ã®å…ç–«é–¢ä¿‚ç ”ç©¶å®¤ï¼ˆè¾²å­¦éƒ¨å«ã‚€ï¼‰
- **è¦æ¨¡**: 500-1000ç ”ç©¶å®¤ï¼ˆç¾åœ¨ã®9ç ”ç©¶å®¤ã‹ã‚‰å¤§å¹…æ‹¡å¼µï¼‰
- **å“è³ª**: æ­£ç¢ºã§æœ€æ–°ã®ç ”ç©¶å†…å®¹æƒ…å ± + ç·åˆå‹é¸æŠœæƒ…å ±

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æˆ¦ç•¥

### 1. ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
```
å„ªå…ˆåº¦1: å„å¤§å­¦å…¬å¼ã‚µã‚¤ãƒˆ
â”œâ”€â”€ åŒ»å­¦éƒ¨ãƒ»åŒ»å­¦ç ”ç©¶ç§‘ã®ç ”ç©¶å®¤ä¸€è¦§
â”œâ”€â”€ ç†å­¦éƒ¨ãƒ»ç”Ÿç‰©å­¦ç§‘ã®ç ”ç©¶å®¤æƒ…å ±
â”œâ”€â”€ å·¥å­¦éƒ¨ãƒ»ãƒã‚¤ã‚ªå·¥å­¦ç§‘ã®ç ”ç©¶å®¤
â”œâ”€â”€ è–¬å­¦éƒ¨ã®ç ”ç©¶å®¤æƒ…å ±
â”œâ”€â”€ è¾²å­¦éƒ¨ãƒ»ç£åŒ»å­¦éƒ¨ã®ç ”ç©¶å®¤æƒ…å ± â˜…æ–°è¦è¿½åŠ 
â”œâ”€â”€ æ­¯å­¦éƒ¨ã®ç ”ç©¶å®¤æƒ…å ±
â””â”€â”€ å…¥è©¦æƒ…å ±ï¼ˆç·åˆå‹é¸æŠœï¼‰ â˜…æ–°è¦è¿½åŠ 

å„ªå…ˆåº¦2: ç ”ç©¶è€…ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
â”œâ”€â”€ researchmap.jp (ç§‘å­¦æŠ€è¡“æŒ¯èˆˆæ©Ÿæ§‹)
â”œâ”€â”€ KAKEN (ç§‘ç ”è²»ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)
â”œâ”€â”€ J-GLOBAL (JST)
â”œâ”€â”€ æ—¥æœ¬å…ç–«å­¦ä¼š ä¼šå“¡æƒ…å ±
â”œâ”€â”€ æ—¥æœ¬ç£åŒ»å­¦ä¼š ä¼šå“¡æƒ…å ± â˜…æ–°è¦è¿½åŠ 
â””â”€â”€ æ—¥æœ¬è¾²èŠ¸åŒ–å­¦ä¼š ä¼šå“¡æƒ…å ± â˜…æ–°è¦è¿½åŠ 

å„ªå…ˆåº¦3: å­¦è¡“æƒ…å ±ãƒ»å…¥è©¦æƒ…å ±
â”œâ”€â”€ PubMedè«–æ–‡ãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ å­¦ä¼šç™ºè¡¨æƒ…å ±
â”œâ”€â”€ ç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
â”œâ”€â”€ å„å¤§å­¦å…¥è©¦è¦é … â˜…æ–°è¦è¿½åŠ 
â””â”€â”€ æ–‡éƒ¨ç§‘å­¦çœå…¥è©¦ãƒ‡ãƒ¼ã‚¿ â˜…æ–°è¦è¿½åŠ 
```

### 2. å¯¾è±¡å¤§å­¦ãƒªã‚¹ãƒˆï¼ˆ3å€æ‹¡å¼µï¼šç´„90æ ¡ï¼‰

#### å›½å…¬ç«‹å¤§å­¦ï¼ˆç´„60æ ¡ï¼‰

##### ã€æ—§å¸å¤§ãƒ»é›£é–¢å›½ç«‹ã€‘
```
- æ±äº¬å¤§å­¦ã€äº¬éƒ½å¤§å­¦ã€å¤§é˜ªå¤§å­¦
- æ±åŒ—å¤§å­¦ã€åå¤å±‹å¤§å­¦ã€ä¹å·å¤§å­¦ã€åŒ—æµ·é“å¤§å­¦
- æ±äº¬å·¥æ¥­å¤§å­¦ã€ä¸€æ©‹å¤§å­¦ã€ç­‘æ³¢å¤§å­¦
- ç¥æˆ¸å¤§å­¦ã€æ¨ªæµœå›½ç«‹å¤§å­¦ã€ãŠèŒ¶ã®æ°´å¥³å­å¤§å­¦
```

##### ã€åŒ»ç§‘å¤§å­¦ãƒ»åŒ»å­¦éƒ¨å¼·è±ªã€‘
```
- æ±äº¬åŒ»ç§‘æ­¯ç§‘å¤§å­¦ã€æµœæ¾åŒ»ç§‘å¤§å­¦
- æ»‹è³€åŒ»ç§‘å¤§å­¦ã€æ—­å·åŒ»ç§‘å¤§å­¦
- å³¶æ ¹å¤§å­¦åŒ»å­¦éƒ¨ã€å±±æ¢¨å¤§å­¦åŒ»å­¦éƒ¨
- å¾³å³¶å¤§å­¦åŒ»å­¦éƒ¨ã€é¦™å·å¤§å­¦åŒ»å­¦éƒ¨
- é«˜çŸ¥å¤§å­¦åŒ»å­¦éƒ¨ã€å¤§åˆ†å¤§å­¦åŒ»å­¦éƒ¨
- å®®å´å¤§å­¦åŒ»å­¦éƒ¨ã€é¹¿å…å³¶å¤§å­¦åŒ»å­¦éƒ¨
```

##### ã€è¾²å­¦ãƒ»ç£åŒ»å­¦å¼·è±ªã€‘â˜…æ–°è¦è¿½åŠ 
```
- æ±äº¬è¾²å·¥å¤§å­¦ã€å²é˜œå¤§å­¦ï¼ˆç£åŒ»å­¦éƒ¨ï¼‰
- é³¥å–å¤§å­¦ï¼ˆè¾²å­¦éƒ¨ãƒ»ç£åŒ»å­¦ç§‘ï¼‰
- å±±å£å¤§å­¦ï¼ˆç£åŒ»å­¦ç§‘ï¼‰
- å®®å´å¤§å­¦ï¼ˆè¾²å­¦éƒ¨ãƒ»ç£åŒ»å­¦ç§‘ï¼‰
- é¹¿å…å³¶å¤§å­¦ï¼ˆç£åŒ»å­¦éƒ¨ï¼‰
- åŒ—æµ·é“å¤§å­¦ï¼ˆç£åŒ»å­¦éƒ¨ãƒ»è¾²å­¦éƒ¨ï¼‰
- å¸¯åºƒç•œç”£å¤§å­¦
```

##### ã€åœ°æ–¹å›½ç«‹å¤§å­¦ã€‘
```
é–¢æ±: èŒ¨åŸå¤§å­¦ã€å®‡éƒ½å®®å¤§å­¦ã€ç¾¤é¦¬å¤§å­¦ã€åŸ¼ç‰å¤§å­¦ã€åƒè‘‰å¤§å­¦ã€
      ä¿¡å·å¤§å­¦ã€æ–°æ½Ÿå¤§å­¦ã€å¯Œå±±å¤§å­¦ã€é‡‘æ²¢å¤§å­¦ã€ç¦äº•å¤§å­¦

é–¢è¥¿: æ»‹è³€å¤§å­¦ã€äº¬éƒ½åºœç«‹å¤§å­¦ã€å¤§é˜ªåºœç«‹å¤§å­¦ã€å¥ˆè‰¯å¥³å­å¤§å­¦ã€
      å’Œæ­Œå±±å¤§å­¦ã€å…µåº«çœŒç«‹å¤§å­¦

ä¸­å›½: å²¡å±±å¤§å­¦ã€åºƒå³¶å¤§å­¦ã€å±±å£å¤§å­¦ã€é³¥å–å¤§å­¦ã€å³¶æ ¹å¤§å­¦

å››å›½: å¾³å³¶å¤§å­¦ã€é¦™å·å¤§å­¦ã€æ„›åª›å¤§å­¦ã€é«˜çŸ¥å¤§å­¦

ä¹å·: ç¦å²¡æ•™è‚²å¤§å­¦ã€ä½è³€å¤§å­¦ã€é•·å´å¤§å­¦ã€ç†Šæœ¬å¤§å­¦ã€
      å¤§åˆ†å¤§å­¦ã€å®®å´å¤§å­¦ã€é¹¿å…å³¶å¤§å­¦ã€ç‰çƒå¤§å­¦

æ±åŒ—: å¼˜å‰å¤§å­¦ã€å²©æ‰‹å¤§å­¦ã€ç§‹ç”°å¤§å­¦ã€å±±å½¢å¤§å­¦ã€ç¦å³¶å¤§å­¦

åŒ—æµ·é“: å®¤è˜­å·¥æ¥­å¤§å­¦ã€å°æ¨½å•†ç§‘å¤§å­¦ã€åŒ—è¦‹å·¥æ¥­å¤§å­¦
```

#### ç§ç«‹å¤§å­¦ï¼ˆå…ç–«ãƒ»è¾²å­¦ç ”ç©¶ã§è‘—åï¼šç´„30æ ¡ï¼‰

##### ã€åŒ»ç§‘å¤§å­¦ãƒ»ç·åˆå¤§å­¦åŒ»å­¦éƒ¨ã€‘
```
- æ…¶æ‡‰ç¾©å¡¾å¤§å­¦ã€æ—©ç¨²ç”°å¤§å­¦ã€ä¸Šæ™ºå¤§å­¦
- é †å¤©å ‚å¤§å­¦ã€æ—¥æœ¬åŒ»ç§‘å¤§å­¦ã€æ±äº¬åŒ»ç§‘å¤§å­¦
- æ˜­å’Œå¤§å­¦ã€æ±é‚¦å¤§å­¦ã€æ—¥æœ¬å¤§å­¦åŒ»å­¦éƒ¨
- è–ãƒãƒªã‚¢ãƒ³ãƒŠåŒ»ç§‘å¤§å­¦ã€åŒ—é‡Œå¤§å­¦
- å…µåº«åŒ»ç§‘å¤§å­¦ã€é–¢è¥¿åŒ»ç§‘å¤§å­¦ã€è¿‘ç•¿å¤§å­¦åŒ»å­¦éƒ¨
- ç¦å²¡å¤§å­¦åŒ»å­¦éƒ¨ã€ä¹…ç•™ç±³å¤§å­¦åŒ»å­¦éƒ¨
```

##### ã€è¾²å­¦ãƒ»ç£åŒ»å­¦ãƒ»ãƒã‚¤ã‚ªç³»å¼·è±ªã€‘â˜…æ–°è¦è¿½åŠ 
```
- æ±äº¬è¾²æ¥­å¤§å­¦ã€æ—¥æœ¬ç£åŒ»ç”Ÿå‘½ç§‘å­¦å¤§å­¦
- éº»å¸ƒå¤§å­¦ï¼ˆç£åŒ»å­¦éƒ¨ï¼‰ã€æ—¥æœ¬å¤§å­¦ç”Ÿç‰©è³‡æºç§‘å­¦éƒ¨
- åŒ—é‡Œå¤§å­¦ç£åŒ»å­¦éƒ¨ã€é…ªè¾²å­¦åœ’å¤§å­¦
- å²¡å±±ç†ç§‘å¤§å­¦ç£åŒ»å­¦éƒ¨
```

##### ã€ç†å·¥ç³»ãƒ»ãƒã‚¤ã‚ªå¼·è±ªã€‘
```
- æ±äº¬ç†ç§‘å¤§å­¦ã€ç«‹æ•™å¤§å­¦ã€æ˜æ²»å¤§å­¦
- æ³•æ”¿å¤§å­¦ã€ä¸­å¤®å¤§å­¦ã€é’å±±å­¦é™¢å¤§å­¦
- é–¢è¥¿å¤§å­¦ã€é–¢è¥¿å­¦é™¢å¤§å­¦ã€åŒå¿—ç¤¾å¤§å­¦ã€ç«‹å‘½é¤¨å¤§å­¦
- ååŸå¤§å­¦ã€å—å±±å¤§å­¦ã€ç¦å²¡å·¥æ¥­å¤§å­¦
```

### 3. æ‹¡å¼µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ

#### 3.1 åŸºæœ¬å…ç–«å­¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
```python
IMMUNE_KEYWORDS = [
    # åŸºæœ¬ç”¨èª
    "å…ç–«", "immunology", "immunity",
    "ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼", "allergy", "allergic",
    "ãƒ¯ã‚¯ãƒãƒ³", "vaccine", "vaccination",
    
    # ç´°èƒãƒ»åˆ†å­
    "Tç´°èƒ", "Bç´°èƒ", "æ¨¹çŠ¶ç´°èƒ", "ãƒã‚¯ãƒ­ãƒ•ã‚¡ãƒ¼ã‚¸",
    "æŠ—ä½“", "æŠ—åŸ", "ã‚µã‚¤ãƒˆã‚«ã‚¤ãƒ³", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ­ã‚¤ã‚­ãƒ³",
    
    # ç–¾æ‚£
    "è‡ªå·±å…ç–«", "ãŒã‚“å…ç–«", "æ„ŸæŸ“å…ç–«",
    "å…ç–«ä¸å…¨", "ç‚ç—‡", "ç§»æ¤å…ç–«",
    
    # æŠ€è¡“ãƒ»æ²»ç™‚
    "å…ç–«ç™‚æ³•", "ç´°èƒç™‚æ³•", "éºä¼å­æ²»ç™‚",
    
    # â˜…æ–°è¦è¿½åŠ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    "iPSç´°èƒ", "iPSC", "induced pluripotent stem cell",
    "ã‚ªãƒ¼ãƒˆãƒ•ã‚¡ã‚¸ãƒ¼", "autophagy",
    "NETosis", "neutrophil extracellular traps",
    
    # â˜…è¿½åŠ å…ç–«é–¢ä¿‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ10å€‹ï¼‰
    "è£œä½“ç³»", "complement system",
    "MHC", "major histocompatibility complex", "ä¸»è¦çµ„ç¹”é©åˆæŠ—åŸ",
    "å…ç–«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ", "immune checkpoint",
    "CAR-Tç´°èƒ", "CAR-T cell therapy",
    "å˜ã‚¯ãƒ­ãƒ¼ãƒ³æŠ—ä½“", "monoclonal antibody",
    "å…ç–«è¨˜æ†¶", "immunological memory",
    "è…¸ç®¡å…ç–«", "intestinal immunity",
    "ç²˜è†œå…ç–«", "mucosal immunity", 
    "å…ˆå¤©å…ç–«", "innate immunity",
    "ç²å¾—å…ç–«", "adaptive immunity"
]
```

#### 3.2 è¾²å­¦éƒ¨ç‰¹åŒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â˜…æ–°è¦è¿½åŠ 
```python
AGRICULTURE_IMMUNE_KEYWORDS = [
    # å‹•ç‰©å…ç–«
    "å‹•ç‰©å…ç–«", "veterinary immunology",
    "å®¶ç•œå…ç–«", "livestock immunity",
    "é­šé¡å…ç–«", "fish immunology",
    "æ°´ç”£å…ç–«", "aquatic immunology",
    
    # æ¤ç‰©å…ç–«
    "æ¤ç‰©å…ç–«", "plant immunity",
    "æ¤ç‰©ç—…ç†", "plant pathology",
    "æ¤ç‰©é˜²å¾¡", "plant defense",
    "ç—…å®³æŠµæŠ—æ€§", "disease resistance",
    
    # é£Ÿå“ãƒ»æ „é¤Šå…ç–«
    "é£Ÿå“å…ç–«", "food immunology",
    "æ „é¤Šå…ç–«", "nutritional immunology",
    "æ©Ÿèƒ½æ€§é£Ÿå“", "functional food",
    "ãƒ—ãƒ­ãƒã‚¤ã‚ªãƒ†ã‚£ã‚¯ã‚¹", "probiotics",
    
    # å¾®ç”Ÿç‰©ãƒ»ç™ºé…µ
    "ç™ºé…µå…ç–«", "fermentation immunity",
    "å¾®ç”Ÿç‰©å…ç–«", "microbial immunology",
    "è…¸å†…ç´°èŒ", "gut microbiota"
]
```

## ğŸ“ ç·åˆå‹é¸æŠœæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ  â˜…æ–°æ©Ÿèƒ½

### 4.1 ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
```python
ADMISSION_INFO_SCHEMA = {
    "university_name": str,
    "faculty_name": str,
    "department_name": str,
    "comprehensive_selection": {
        "available": bool,
        "quota": int or str,  # å‹Ÿé›†å®šå“¡ or "-"
        "info_url": str,
        "application_period": str,
        "selection_method": str,
        "last_updated": datetime
    }
}
```

### 4.2 ç·åˆå‹é¸æŠœã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆ¦ç•¥
```python
class ComprehensiveAdmissionScraper:
    """ç·åˆå‹é¸æŠœæƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        self.admission_keywords = [
            "ç·åˆå‹é¸æŠœ", "AOå…¥è©¦", "ç·åˆé¸æŠœ",
            "ç‰¹åˆ¥é¸æŠœ", "æ¨è–¦å…¥è©¦", "è‡ªå·±æ¨è–¦"
        ]
    
    def scrape_admission_info(self, university_url):
        """å¤§å­¦ã®å…¥è©¦æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        admission_pages = self.find_admission_pages(university_url)
        
        for page in admission_pages:
            info = self.parse_admission_page(page)
            if self.is_comprehensive_selection(info):
                return self.extract_quota_and_url(info)
        
        return {"available": False, "quota": "-", "info_url": None}
    
    def parse_admission_page(self, page_url):
        """å…¥è©¦ãƒšãƒ¼ã‚¸ã®è©³ç´°è§£æ"""
        soup = self.get_page_content(page_url)
        
        # å­¦éƒ¨ãƒ»å­¦ç§‘åˆ¥ã®ç·åˆå‹é¸æŠœæƒ…å ±ã‚’æŠ½å‡º
        faculties = soup.find_all(['div', 'section'], 
                                 class_=re.compile(r'faculty|department|admission'))
        
        admission_data = {}
        for faculty in faculties:
            faculty_name = self.extract_faculty_name(faculty)
            
            # ç·åˆå‹é¸æŠœã®å‹Ÿé›†å®šå“¡ã‚’æ¤œç´¢
            quota_text = faculty.get_text()
            quota = self.extract_quota(quota_text)
            
            admission_data[faculty_name] = {
                "quota": quota,
                "details_url": self.find_details_url(faculty)
            }
        
        return admission_data
    
    def extract_quota(self, text):
        """å‹Ÿé›†å®šå“¡ã®æŠ½å‡º"""
        # "è‹¥å¹²å", "5å", "10åç¨‹åº¦" ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        patterns = [
            r'(\d+)å',
            r'(\d+)äºº',
            r'è‹¥å¹²å',
            r'æ•°å',
            r'(\d+)åç¨‹åº¦'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                if pattern in ['è‹¥å¹²å', 'æ•°å']:
                    return 'è‹¥å¹²å'
                return match.group(1) + 'å'
        
        return '-'  # å®Ÿæ–½ã—ã¦ã„ãªã„å ´åˆ
```

## ğŸ› ï¸ æŠ€è¡“å®Ÿè£…æˆ¦ç•¥ï¼ˆæ‹¡å¼µç‰ˆï¼‰

### Phase 1: å¤šå­¦éƒ¨å¯¾å¿œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åŸºç›¤

#### 1.1 æ‹¡å¼µã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
```python
research_lab_scraper/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ university_scraper.py           # å¤§å­¦åˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
â”‚   â”œâ”€â”€ medical_scraper.py              # åŒ»å­¦éƒ¨å°‚ç”¨
â”‚   â”œâ”€â”€ agriculture_scraper.py          # è¾²å­¦éƒ¨å°‚ç”¨ â˜…æ–°è¦
â”‚   â”œâ”€â”€ veterinary_scraper.py           # ç£åŒ»å­¦éƒ¨å°‚ç”¨ â˜…æ–°è¦
â”‚   â”œâ”€â”€ admission_scraper.py            # ç·åˆå‹é¸æŠœå°‚ç”¨ â˜…æ–°è¦
â”‚   â”œâ”€â”€ researchmap_scraper.py          # researchmapå°‚ç”¨
â”‚   â””â”€â”€ kaken_scraper.py                # ç§‘ç ”è²»DBå°‚ç”¨
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ content_parser.py               # ç ”ç©¶å†…å®¹è§£æ
â”‚   â”œâ”€â”€ contact_parser.py               # é€£çµ¡å…ˆæƒ…å ±è§£æ
â”‚   â”œâ”€â”€ agriculture_parser.py           # è¾²å­¦ç³»å†…å®¹è§£æ â˜…æ–°è¦
â”‚   â””â”€â”€ admission_parser.py             # å…¥è©¦æƒ…å ±è§£æ â˜…æ–°è¦
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ university_urls.json            # å¤§å­¦URLä¸€è¦§ï¼ˆ90æ ¡ï¼‰
â”‚   â”œâ”€â”€ medical_keywords.json           # åŒ»å­¦ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ agriculture_keywords.json       # è¾²å­¦ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â˜…æ–°è¦
â”‚   â””â”€â”€ admission_keywords.json         # å…¥è©¦é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ â˜…æ–°è¦
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ models.py                       # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â””â”€â”€ migrations/                     # DBå¤‰æ›´ç®¡ç†
â””â”€â”€ utils/
    â”œâ”€â”€ rate_limiter.py                 # ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
    â”œâ”€â”€ data_validator.py               # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    â””â”€â”€ url_discovery.py                # URLè‡ªå‹•ç™ºè¦‹
```

#### 1.2 æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
```python
# ç ”ç©¶å®¤ãƒ†ãƒ¼ãƒ–ãƒ«æ‹¡å¼µ
class ResearchLab(Base):
    __tablename__ = "research_labs"
    
    # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    id = Column(Integer, primary_key=True)
    university_id = Column(Integer, ForeignKey("universities.id"))
    name = Column(String(255))
    professor_name = Column(String(255))
    department = Column(String(255))
    faculty = Column(String(255))  # â˜…æ–°è¦è¿½åŠ ï¼šå­¦éƒ¨
    research_theme = Column(Text)
    research_content = Column(Text)
    research_field = Column(String(100))
    
    # â˜…æ–°è¦è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    lab_type = Column(String(50))  # medical, agriculture, veterinary, etc.
    animal_species = Column(String(255))  # å‹•ç‰©ç¨®ï¼ˆç£åŒ»ãƒ»ç•œç”£ç³»ï¼‰
    plant_species = Column(String(255))   # æ¤ç‰©ç¨®ï¼ˆè¾²å­¦ç³»ï¼‰
    research_techniques = Column(Text)     # ç ”ç©¶æŠ€è¡“ãƒ»æ‰‹æ³•
    
    # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    embedding = Column(Vector(1536))

# â˜…æ–°è¦è¿½åŠ ï¼šç·åˆå‹é¸æŠœãƒ†ãƒ¼ãƒ–ãƒ«
class ComprehensiveAdmission(Base):
    __tablename__ = "comprehensive_admissions"
    
    id = Column(Integer, primary_key=True)
    university_id = Column(Integer, ForeignKey("universities.id"))
    faculty = Column(String(255), nullable=False)
    department = Column(String(255))
    is_available = Column(Boolean, default=False)
    quota = Column(String(50))  # "10å", "è‹¥å¹²å", "-"
    info_url = Column(String(500))
    application_period = Column(String(255))
    selection_method = Column(Text)
    last_updated = Column(DateTime, default=datetime.utcnow)
```

### Phase 2: å¤§å­¦åˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè£…ï¼ˆæ‹¡å¼µç‰ˆï¼‰

#### 2.1 æ±äº¬è¾²å·¥å¤§å­¦ã®ä¾‹ â˜…æ–°è¦
```python
class TokyoUniversityAgricultureScraper:
    """æ±äº¬è¾²å·¥å¤§å­¦ è¾²å­¦éƒ¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    BASE_URL = "https://www.tuat.ac.jp"
    
    def scrape_agriculture_labs(self):
        """è¾²å­¦éƒ¨ç ”ç©¶å®¤ä¸€è¦§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        urls = [
            f"{self.BASE_URL}/faculty/agriculture/departments/",
            f"{self.BASE_URL}/graduate/agriculture/research/"
        ]
        
        labs = []
        for url in urls:
            page_labs = self.parse_lab_pages(url)
            labs.extend(self.filter_immune_related(page_labs))
        
        return labs
    
    def scrape_veterinary_labs(self):
        """ç£åŒ»å­¦é–¢é€£ç ”ç©¶å®¤"""
        vet_url = f"{self.BASE_URL}/faculty/agriculture/veterinary/"
        return self.parse_vet_labs(vet_url)
    
    def filter_immune_related(self, labs):
        """å…ç–«é–¢é€£ç ”ç©¶å®¤ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        immune_labs = []
        
        for lab in labs:
            content = lab.get('research_content', '').lower()
            
            # è¾²å­¦ç³»å…ç–«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            if any(keyword.lower() in content for keyword in AGRICULTURE_IMMUNE_KEYWORDS):
                lab['lab_type'] = 'agriculture_immune'
                immune_labs.append(lab)
            
            # å‹•ç‰©ç¨®æƒ…å ±ã®æŠ½å‡º
            animals = self.extract_animal_species(content)
            if animals:
                lab['animal_species'] = ', '.join(animals)
        
        return immune_labs
    
    def extract_animal_species(self, content):
        """ç ”ç©¶å¯¾è±¡å‹•ç‰©ç¨®ã®æŠ½å‡º"""
        animal_keywords = [
            'ç‰›', 'cattle', 'è±š', 'pig', 'swine',
            'é¶', 'chicken', 'é­š', 'fish',
            'ãƒã‚¦ã‚¹', 'mouse', 'ãƒ©ãƒƒãƒˆ', 'rat',
            'ç¾Š', 'sheep', 'å±±ç¾Š', 'goat'
        ]
        
        found_animals = []
        for animal in animal_keywords:
            if animal in content.lower():
                found_animals.append(animal)
        
        return found_animals
```

#### 2.2 ç·åˆå‹é¸æŠœæƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè£…
```python
class UniversityAdmissionScraper:
    """å¤§å­¦åˆ¥ç·åˆå‹é¸æŠœæƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def scrape_comprehensive_admission(self, university_name, base_url):
        """ç·åˆå‹é¸æŠœæƒ…å ±ã‚’å–å¾—"""
        
        # å…¥è©¦æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        admission_urls = self.find_admission_pages(base_url)
        
        admission_data = {}
        
        for url in admission_urls:
            try:
                page_data = self.parse_admission_page(url)
                admission_data.update(page_data)
            except Exception as e:
                logger.warning(f"Failed to parse {url}: {e}")
        
        return self.format_admission_data(admission_data)
    
    def find_admission_pages(self, base_url):
        """å…¥è©¦é–¢é€£ãƒšãƒ¼ã‚¸ã®URLç™ºè¦‹"""
        search_paths = [
            '/admission/', '/nyushi/', '/entrance/',
            '/undergraduate/admission/', '/graduate/admission/',
            '/faculty/*/admission/', '/å…¥è©¦/'
        ]
        
        found_urls = []
        for path in search_paths:
            potential_url = urljoin(base_url, path)
            if self.url_exists(potential_url):
                found_urls.append(potential_url)
        
        return found_urls
    
    def parse_admission_page(self, url):
        """å…¥è©¦ãƒšãƒ¼ã‚¸ã®è©³ç´°è§£æ"""
        soup = self.get_page_content(url)
        
        # å­¦éƒ¨ãƒ»å­¦ç§‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç‰¹å®š
        sections = soup.find_all(['div', 'section', 'table'], 
                                class_=re.compile(r'faculty|department|course'))
        
        admission_info = {}
        
        for section in sections:
            # å­¦éƒ¨ãƒ»å­¦ç§‘åã‚’æŠ½å‡º
            faculty_info = self.extract_faculty_info(section)
            
            if faculty_info:
                # ç·åˆå‹é¸æŠœã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
                comprehensive_info = self.check_comprehensive_selection(section)
                
                admission_info[faculty_info['name']] = {
                    'department': faculty_info.get('department'),
                    'comprehensive_selection': comprehensive_info
                }
        
        return admission_info
    
    def check_comprehensive_selection(self, section):
        """ç·åˆå‹é¸æŠœå®Ÿæ–½çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        text = section.get_text()
        
        # ç·åˆå‹é¸æŠœã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        comprehensive_keywords = ['ç·åˆå‹é¸æŠœ', 'AOå…¥è©¦', 'ç·åˆé¸æŠœ']
        
        if any(keyword in text for keyword in comprehensive_keywords):
            quota = self.extract_quota(text)
            details_url = self.find_details_url(section)
            
            return {
                'available': True,
                'quota': quota,
                'info_url': details_url
            }
        
        return {
            'available': False,
            'quota': '-',
            'info_url': None
        }
```

## ğŸ“‹ å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ‹¡å¼µç‰ˆï¼‰

### Week 1-2: æ‹¡å¼µåŸºç›¤æ§‹ç¯‰
- [ ] å¤šå­¦éƒ¨å¯¾å¿œã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
- [ ] 90å¤§å­¦URLåé›†ãƒ»æ•´ç†
- [ ] è¾²å­¦ç³»ãƒ»ç·åˆå‹é¸æŠœã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ä½œæˆ
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µãƒ»ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### Week 3-4: ä¸»è¦å¤§å­¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆåŒ»å­¦ãƒ»è¾²å­¦ï¼‰
- [ ] æ—§å¸å¤§7æ ¡ã®åŒ»å­¦éƒ¨ãƒ»è¾²å­¦éƒ¨ç ”ç©¶å®¤
- [ ] è¾²å·¥å¤§ãƒ»åŒ—å¤§ãƒ»å¸¯åºƒç•œç”£å¤§ã®è¾²å­¦ãƒ»ç£åŒ»å­¦éƒ¨
- [ ] ç·åˆå‹é¸æŠœæƒ…å ±ã®ä¸¦è¡Œåé›†

### Week 5-6: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆãƒ»å“è³ªç®¡ç†
- [ ] ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®DBæŠ•å…¥
- [ ] é‡è¤‡æ’é™¤ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- [ ] ç·åˆå‹é¸æŠœãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ

### Week 7-8: æ‹¡å¼µãƒ»ç§ç«‹å¤§å­¦è¿½åŠ 
- [ ] ç§ç«‹å¤§å­¦30æ ¡ã®è¿½åŠ 
- [ ] researchmap.jpé€£æºå¼·åŒ–
- [ ] è‡ªå‹•æ›´æ–°ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…

### Week 9-10: å“è³ªå‘ä¸Šãƒ»æ©Ÿèƒ½æ‹¡å¼µ
- [ ] æ¤œç´¢ç²¾åº¦å‘ä¸Š
- [ ] ç·åˆå‹é¸æŠœãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
- [ ] ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ãƒ»çµ±è¨ˆæ©Ÿèƒ½

## ğŸ¯ æœŸå¾…ã•ã‚Œã‚‹æˆæœï¼ˆæ‹¡å¼µç‰ˆï¼‰

### æ•°å€¤ç›®æ¨™
- **ç ”ç©¶å®¤æ•°**: 500-1000ä»¶ï¼ˆç¾åœ¨ã®9ä»¶ã‹ã‚‰100å€æ‹¡å¼µï¼‰
- **å¤§å­¦æ•°**: 90æ ¡ï¼ˆ3å€æ‹¡å¼µï¼‰
- **å­¦éƒ¨ã‚«ãƒãƒ¬ãƒƒã‚¸**: åŒ»å­¦ãƒ»ç†å­¦ãƒ»å·¥å­¦ãƒ»è–¬å­¦ãƒ»è¾²å­¦ãƒ»ç£åŒ»å­¦
- **ç·åˆå‹é¸æŠœæƒ…å ±**: 90æ ¡Ã—å¹³å‡5å­¦éƒ¨ = 450å­¦éƒ¨ã®å…¥è©¦æƒ…å ±

### æ–°æ©Ÿèƒ½
- **è¾²å­¦ãƒ»ç£åŒ»å­¦æ¤œç´¢**: å‹•ç‰©ãƒ»æ¤ç‰©ãƒ»é£Ÿå“å…ç–«ç ”ç©¶ã®ç™ºè¦‹
- **ç·åˆå‹é¸æŠœãƒ•ã‚£ãƒ«ã‚¿**: ã€Œç·åˆå‹é¸æŠœã‚ã‚Šã€ã§çµã‚Šè¾¼ã¿å¯èƒ½
- **å‹Ÿé›†å®šå“¡è¡¨ç¤º**: å„å­¦éƒ¨ã®ç·åˆå‹é¸æŠœå®šå“¡ã‚’è¡¨ç¤º
- **å…¥è©¦æƒ…å ±ãƒªãƒ³ã‚¯**: è©³ç´°ãªå…¥è©¦æƒ…å ±ã¸ã®ç›´æ¥ãƒªãƒ³ã‚¯

ã“ã®æ‹¡å¼µæˆ¦ç•¥ã§å®Ÿè£…ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿç‰¹ã«æ³¨åŠ›ã—ãŸã„éƒ¨åˆ†ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ