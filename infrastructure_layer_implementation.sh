#!/bin/bash

echo "ğŸ—ï¸ Infrastructureå±¤ï¼ˆæŠ€è¡“åŸºç›¤ï¼‰ã‚’å®Ÿè£…ä¸­..."

# ==================== 1. HTTPåŸºç›¤å®Ÿè£… ====================
echo "ğŸŒ HTTPåŸºç›¤ã‚’å®Ÿè£…ä¸­..."

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å™¨
cat > scraper/infrastructure/http/rate_limiter.py << 'EOF'
"""
éåŒæœŸãƒ¬ãƒ¼ãƒˆåˆ¶é™å™¨
å¤§å­¦ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·ã‚’é©åˆ‡ã«åˆ¶å¾¡
"""

import asyncio
import time
from typing import Dict, Optional
from dataclasses import dataclass

from scraper.config.interfaces import RateLimiterInterface


@dataclass
class RateLimitConfig:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š"""
    requests_per_second: float = 0.5
    burst_limit: int = 3
    window_size: int = 60


class TokenBucketRateLimiter:
    """
    ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ãƒ¬ãƒ¼ãƒˆåˆ¶é™
    å¤§å­¦åˆ¥ã«å€‹åˆ¥ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é©ç”¨
    """
    
    def __init__(self, config: RateLimitConfig):
        self._config = config
        self._buckets: Dict[str, Dict] = {}
        self._lock = asyncio.Lock()
    
    async def acquire(self, domain: str = "default") -> None:
        """
        ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨±å¯ã‚’å–å¾—
        
        Args:
            domain: å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆå¤§å­¦åˆ¥åˆ¶é™ç”¨ï¼‰
        """
        async with self._lock:
            now = time.time()
            
            if domain not in self._buckets:
                self._buckets[domain] = {
                    'tokens': self._config.burst_limit,
                    'last_refill': now
                }
            
            bucket = self._buckets[domain]
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã®è£œå……
            time_passed = now - bucket['last_refill']
            tokens_to_add = time_passed * self._config.requests_per_second
            bucket['tokens'] = min(
                self._config.burst_limit,
                bucket['tokens'] + tokens_to_add
            )
            bucket['last_refill'] = now
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯å¾…æ©Ÿ
            if bucket['tokens'] < 1:
                wait_time = (1 - bucket['tokens']) / self._config.requests_per_second
                await asyncio.sleep(wait_time)
                bucket['tokens'] = 0
            else:
                bucket['tokens'] -= 1
    
    def get_delay(self, domain: str = "default") -> float:
        """æ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ã®æ¨å¥¨å¾…æ©Ÿæ™‚é–“ã‚’å–å¾—"""
        if domain not in self._buckets:
            return 0.0
        
        bucket = self._buckets[domain]
        if bucket['tokens'] >= 1:
            return 0.0
        
        return (1 - bucket['tokens']) / self._config.requests_per_second
    
    def reset_bucket(self, domain: str) -> None:
        """æŒ‡å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒã‚±ãƒƒãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ"""
        if domain in self._buckets:
            del self._buckets[domain]


class AdaptiveRateLimiter:
    """
    é©å¿œçš„ãƒ¬ãƒ¼ãƒˆåˆ¶é™å™¨
    ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«åŸºã¥ã„ã¦å‹•çš„ã«åˆ¶é™ã‚’èª¿æ•´
    """
    
    def __init__(self, base_config: RateLimitConfig):
        self._base_config = base_config
        self._current_rate = base_config.requests_per_second
        self._success_count = 0
        self._error_count = 0
        self._last_adjustment = time.time()
        self._bucket_limiter = TokenBucketRateLimiter(base_config)
    
    async def acquire(self, domain: str = "default") -> None:
        """é©å¿œçš„ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨±å¯ã‚’å–å¾—"""
        # ç¾åœ¨ã®ãƒ¬ãƒ¼ãƒˆã§ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆåˆ¶é™
        await self._bucket_limiter.acquire(domain)
        
        # å®šæœŸçš„ã«ãƒ¬ãƒ¼ãƒˆèª¿æ•´
        await self._adjust_rate_if_needed()
    
    async def _adjust_rate_if_needed(self) -> None:
        """ã‚¨ãƒ©ãƒ¼ç‡ã«åŸºã¥ã„ã¦ãƒ¬ãƒ¼ãƒˆã‚’èª¿æ•´"""
        now = time.time()
        
        # 1åˆ†ã”ã¨ã«èª¿æ•´
        if now - self._last_adjustment < 60:
            return
        
        total_requests = self._success_count + self._error_count
        if total_requests < 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            return
        
        error_rate = self._error_count / total_requests
        
        if error_rate > 0.1:  # ã‚¨ãƒ©ãƒ¼ç‡10%è¶…éã§ãƒ¬ãƒ¼ãƒˆå‰Šæ¸›
            self._current_rate *= 0.8
            print(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¼·åŒ–: {self._current_rate:.2f} req/s (ã‚¨ãƒ©ãƒ¼ç‡: {error_rate:.2%})")
        elif error_rate < 0.02:  # ã‚¨ãƒ©ãƒ¼ç‡2%æœªæº€ã§ãƒ¬ãƒ¼ãƒˆç·©å’Œ
            self._current_rate = min(
                self._current_rate * 1.1,
                self._base_config.requests_per_second * 2  # æœ€å¤§ã§åŸºæº–å€¤ã®2å€ã¾ã§
            )
            print(f"âœ… ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç·©å’Œ: {self._current_rate:.2f} req/s")
        
        # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
        self._success_count = 0
        self._error_count = 0
        self._last_adjustment = now
        
        # ãƒã‚±ãƒƒãƒˆè¨­å®šæ›´æ–°
        new_config = RateLimitConfig(
            requests_per_second=self._current_rate,
            burst_limit=self._base_config.burst_limit,
            window_size=self._base_config.window_size
        )
        self._bucket_limiter = TokenBucketRateLimiter(new_config)
    
    def record_success(self) -> None:
        """æˆåŠŸã‚’è¨˜éŒ²"""
        self._success_count += 1
    
    def record_error(self) -> None:
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        self._error_count += 1
EOF

# HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
cat > scraper/infrastructure/http/http_client.py << 'EOF'
"""
å …ç‰¢ãªHTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
å†è©¦è¡Œã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç®¡ç†
"""

import aiohttp
import asyncio
import logging
from typing import Optional, Dict, Any, Union
from urllib.parse import urljoin, urlparse
import random

from scraper.config.settings import scraping_settings
from scraper.infrastructure.http.rate_limiter import AdaptiveRateLimiter, RateLimitConfig
from scraper.infrastructure.http.retry_handler import RetryHandler

logger = logging.getLogger(__name__)


class ResearchLabHttpClient:
    """ç ”ç©¶å®¤ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å°‚ç”¨HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self._session: Optional[aiohttp.ClientSession] = None
        self._rate_limiter = AdaptiveRateLimiter(
            RateLimitConfig(
                requests_per_second=scraping_settings.requests_per_second,
                burst_limit=scraping_settings.concurrent_requests
            )
        )
        self._retry_handler = RetryHandler()
        self._user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        await self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        await self.close()
    
    async def _ensure_session(self) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç¢ºä¿"""
        if self._session is None or self._session.closed:
            timeout = aiohttp.ClientTimeout(total=scraping_settings.request_timeout)
            connector = aiohttp.TCPConnector(
                limit=scraping_settings.concurrent_requests,
                limit_per_host=2,  # åŒä¸€ãƒ›ã‚¹ãƒˆã¸ã®åŒæ™‚æ¥ç¶šæ•°åˆ¶é™
                ttl_dns_cache=300,  # DNS ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                use_dns_cache=True
            )
            
            self._session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector,
                headers={'User-Agent': random.choice(self._user_agents)}
            )
    
    async def get(
        self, 
        url: str, 
        headers: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> aiohttp.ClientResponse:
        """
        HTTPGETãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»å†è©¦è¡Œä»˜ãï¼‰
        
        Args:
            url: ãƒªã‚¯ã‚¨ã‚¹ãƒˆURL
            headers: è¿½åŠ ãƒ˜ãƒƒãƒ€ãƒ¼
            **kwargs: ãã®ä»–ã®aiohttpå¼•æ•°
        
        Returns:
            aiohttp.ClientResponse: ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        await self._ensure_session()
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³æŠ½å‡º
        domain = urlparse(url).netloc
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©ç”¨
        await self._rate_limiter.acquire(domain)
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œï¼ˆå†è©¦è¡Œä»˜ãï¼‰
        return await self._retry_handler.execute_with_retry(
            self._make_request, url, headers, **kwargs
        )
    
    async def _make_request(
        self, 
        url: str, 
        headers: Optional[Dict[str, str]] = None,
        **kwargs
    ) -> aiohttp.ClientResponse:
        """å®Ÿéš›ã®HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        request_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        if headers:
            request_headers.update(headers)
        
        logger.debug(f"Requesting: {url}")
        
        try:
            response = await self._session.get(url, headers=request_headers, **kwargs)
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹çŠ¶æ…‹è¨˜éŒ²
            if response.status < 400:
                self._rate_limiter.record_success()
                logger.debug(f"Success: {url} ({response.status})")
            else:
                self._rate_limiter.record_error()
                logger.warning(f"HTTP Error: {url} ({response.status})")
            
            return response
            
        except Exception as e:
            self._rate_limiter.record_error()
            logger.error(f"Request failed: {url} - {e}")
            raise
    
    async def get_text(self, url: str, encoding: str = 'utf-8', **kwargs) -> str:
        """HTMLãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        response = await self.get(url, **kwargs)
        try:
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
            if response.charset:
                encoding = response.charset
            
            content = await response.text(encoding=encoding)
            return content
        finally:
            response.close()
    
    async def get_multiple(
        self, 
        urls: list[str], 
        max_concurrent: int = 3
    ) -> list[Union[str, Exception]]:
        """è¤‡æ•°URLã®ä¸¦è¡Œå–å¾—"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def fetch_single(url: str):
            async with semaphore:
                try:
                    return await self.get_text(url)
                except Exception as e:
                    logger.error(f"Failed to fetch {url}: {e}")
                    return e
        
        tasks = [fetch_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    async def close(self) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒ­ãƒ¼ã‚º"""
        if self._session and not self._session.closed:
            await self._session.close()
EOF

# å†è©¦è¡Œãƒãƒ³ãƒ‰ãƒ©ãƒ¼
cat > scraper/infrastructure/http/retry_handler.py << 'EOF'
"""
å …ç‰¢ãªå†è©¦è¡Œæ©Ÿæ§‹
ä¸€æ™‚çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¨ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œ
"""

import asyncio
import logging
from typing import Any, Callable, TypeVar, Optional
import random
from functools import wraps

from scraper.config.settings import scraping_settings
from scraper.config.interfaces import ScrapingError

logger = logging.getLogger(__name__)

T = TypeVar('T')


class RetryHandler:
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã‚’ä½¿ç”¨ã—ãŸå†è©¦è¡Œãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(
        self,
        max_retries: int = None,
        base_delay: float = None,
        max_delay: float = 60.0,
        backoff_factor: float = 2.0,
        jitter: bool = True
    ):
        self.max_retries = max_retries or scraping_settings.retry_attempts
        self.base_delay = base_delay or scraping_settings.retry_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor
        self.jitter = jitter
    
    async def execute_with_retry(
        self,
        func: Callable[..., T],
        *args,
        **kwargs
    ) -> T:
        """
        æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹å†è©¦è¡Œå®Ÿè¡Œ
        
        Args:
            func: å®Ÿè¡Œã™ã‚‹é–¢æ•°
            *args: é–¢æ•°ã®å¼•æ•°
            **kwargs: é–¢æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°
        
        Returns:
            T: é–¢æ•°ã®æˆ»ã‚Šå€¤
        
        Raises:
            ScrapingError: æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ãŸå ´åˆ
        """
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                result = await func(*args, **kwargs)
                
                if attempt > 0:
                    logger.info(f"âœ… Retry succeeded on attempt {attempt + 1}")
                
                return result
                
            except Exception as e:
                last_exception = e
                
                # å†è©¦è¡Œä¸å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å³åº§ã«å¤±æ•—
                if not self._is_retryable_error(e):
                    logger.error(f"âŒ Non-retryable error: {e}")
                    raise
                
                # æœ€çµ‚è©¦è¡Œã®å ´åˆã¯å¤±æ•—
                if attempt >= self.max_retries:
                    logger.error(f"âŒ Max retries ({self.max_retries}) exceeded")
                    break
                
                # å¾…æ©Ÿæ™‚é–“è¨ˆç®—
                delay = self._calculate_delay(attempt)
                logger.warning(
                    f"âš ï¸ Attempt {attempt + 1} failed: {e}. "
                    f"Retrying in {delay:.2f}s..."
                )
                
                await asyncio.sleep(delay)
        
        # å…¨è©¦è¡Œå¤±æ•—
        raise ScrapingError(
            f"Failed after {self.max_retries} retries. Last error: {last_exception}"
        )
    
    def _is_retryable_error(self, error: Exception) -> bool:
        """ã‚¨ãƒ©ãƒ¼ãŒå†è©¦è¡Œå¯èƒ½ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        import aiohttp
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯å†è©¦è¡Œå¯èƒ½
        if isinstance(error, (
            aiohttp.ClientConnectionError,
            aiohttp.ClientTimeout,
            asyncio.TimeoutError,
            ConnectionError
        )):
            return True
        
        # HTTPã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã§åˆ¤å®š
        if isinstance(error, aiohttp.ClientResponseError):
            # 5xx ã‚¨ãƒ©ãƒ¼ï¼ˆã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ï¼‰ã¯å†è©¦è¡Œå¯èƒ½
            if 500 <= error.status < 600:
                return True
            # 429 (Too Many Requests) ã‚‚å†è©¦è¡Œå¯èƒ½
            if error.status == 429:
                return True
            # 4xx ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ï¼‰ã¯å†è©¦è¡Œä¸å¯
            return False
        
        # ãã®ä»–ã®ä¾‹å¤–ã¯å†è©¦è¡Œä¸å¯
        return False
    
    def _calculate_delay(self, attempt: int) -> float:
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹å¾…æ©Ÿæ™‚é–“è¨ˆç®—"""
        delay = self.base_delay * (self.backoff_factor ** attempt)
        delay = min(delay, self.max_delay)
        
        # ã‚¸ãƒƒã‚¿ãƒ¼è¿½åŠ ï¼ˆåŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åˆ†æ•£ï¼‰
        if self.jitter:
            delay *= (0.5 + random.random() * 0.5)
        
        return delay


def retry_on_failure(
    max_retries: int = 3,
    base_delay: float = 1.0,
    backoff_factor: float = 2.0
):
    """å†è©¦è¡Œãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            handler = RetryHandler(
                max_retries=max_retries,
                base_delay=base_delay,
                backoff_factor=backoff_factor
            )
            return await handler.execute_with_retry(func, *args, **kwargs)
        return wrapper
    return decorator
EOF

echo "âœ… HTTPåŸºç›¤å®Ÿè£…å®Œäº†"

# ==================== 2. ãƒ‘ãƒ¼ã‚µãƒ¼åŸºç›¤å®Ÿè£… ====================
echo "ğŸ“ ãƒ‘ãƒ¼ã‚µãƒ¼åŸºç›¤ã‚’å®Ÿè£…ä¸­..."

# åŸºåº•ãƒ‘ãƒ¼ã‚µãƒ¼
cat > scraper/infrastructure/parsers/base_parser.py << 'EOF'
"""
HTMLè§£æåŸºåº•ã‚¯ãƒ©ã‚¹
å…±é€šã®ãƒ‘ãƒ¼ã‚¹æ©Ÿèƒ½ã‚’æä¾›
"""

from abc import ABC, abstractmethod
from bs4 import BeautifulSoup, Tag
from typing import Dict, List, Optional, Union
import re
import logging

logger = logging.getLogger(__name__)


class BaseHtmlParser(ABC):
    """HTMLè§£æåŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, encoding: str = 'utf-8'):
        self.encoding = encoding
        self._soup: Optional[BeautifulSoup] = None
    
    def parse_html(self, html_content: str) -> BeautifulSoup:
        """HTMLã‚’ãƒ‘ãƒ¼ã‚¹"""
        self._soup = BeautifulSoup(html_content, 'lxml')
        return self._soup
    
    def find_by_text_patterns(
        self, 
        patterns: List[str], 
        tag_types: List[str] = None
    ) -> List[Tag]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¿ã‚°ã‚’æ¤œç´¢"""
        if not self._soup:
            return []
        
        found_tags = []
        tag_types = tag_types or ['div', 'p', 'span', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']
        
        for pattern in patterns:
            regex = re.compile(pattern, re.IGNORECASE)
            for tag_type in tag_types:
                tags = self._soup.find_all(tag_type, string=regex)
                found_tags.extend(tags)
        
        return found_tags
    
    def extract_text_content(self, element: Union[Tag, BeautifulSoup]) -> str:
        """è¦ç´ ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãï¼‰"""
        if not element:
            return ""
        
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        text = element.get_text(separator=' ', strip=True)
        
        # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = re.sub(r'\s+', ' ', text)  # é€£ç¶šç©ºç™½ã‚’å˜ä¸€ç©ºç™½ã«
        text = re.sub(r'\n+', '\n', text)  # é€£ç¶šæ”¹è¡Œã‚’å˜ä¸€æ”¹è¡Œã«
        text = text.strip()
        
        return text
    
    def find_contact_info(self) -> Dict[str, str]:
        """é€£çµ¡å…ˆæƒ…å ±ã‚’æŠ½å‡º"""
        contact_info = {}
        
        if not self._soup:
            return contact_info
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æ¤œç´¢
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        text_content = self._soup.get_text()
        emails = re.findall(email_pattern, text_content)
        if emails:
            contact_info['email'] = emails[0]
        
        # é›»è©±ç•ªå·æ¤œç´¢
        phone_patterns = [
            r'\b0\d{1,4}-\d{1,4}-\d{4}\b',  # æ—¥æœ¬ã®é›»è©±ç•ªå·
            r'\b\d{3}-\d{3}-\d{4}\b',       # çŸ­ç¸®å½¢
            r'\b\(\d{3}\)\s*\d{3}-\d{4}\b'  # (03) 1234-5678
        ]
        
        for pattern in phone_patterns:
            phones = re.findall(pattern, text_content)
            if phones:
                contact_info['phone'] = phones[0]
                break
        
        return contact_info
    
    def find_links_by_text(self, link_texts: List[str]) -> List[str]:
        """æŒ‡å®šãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢"""
        if not self._soup:
            return []
        
        found_links = []
        for link_text in link_texts:
            links = self._soup.find_all('a', string=re.compile(link_text, re.IGNORECASE))
            for link in links:
                href = link.get('href')
                if href:
                    found_links.append(href)
        
        return found_links
    
    def extract_keywords_from_meta(self) -> List[str]:
        """metaã‚¿ã‚°ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        if not self._soup:
            return []
        
        keywords = []
        
        # meta keywords
        meta_keywords = self._soup.find('meta', {'name': 'keywords'})
        if meta_keywords and meta_keywords.get('content'):
            keywords.extend([kw.strip() for kw in meta_keywords['content'].split(',')])
        
        # meta description
        meta_desc = self._soup.find('meta', {'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚ŠãŒã‚ã‚‹å ´åˆï¼‰
            desc_content = meta_desc['content']
            if ',' in desc_content:
                keywords.extend([kw.strip() for kw in desc_content.split(',')[:5]])
        
        return list(set(keywords))  # é‡è¤‡é™¤å»
    
    @abstractmethod
    def extract_research_labs(self) -> List[Dict[str, str]]:
        """ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡ºï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        pass
    
    def clean_professor_name(self, name: str) -> str:
        """æ•™æˆåã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not name:
            return ""
        
        # æ•¬ç§°ã‚’é™¤å»
        name = re.sub(r'(æ•™æˆ|å‡†æ•™æˆ|è¬›å¸«|åŠ©æ•™|åšå£«|Dr\.|Prof\.)', '', name)
        name = re.sub(r'\s+', ' ', name).strip()
        
        return name
    
    def clean_department_name(self, dept: str) -> str:
        """å­¦éƒ¨ãƒ»å­¦ç§‘åã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not dept:
            return ""
        
        # ä¸è¦ãªæ–‡å­—ã‚’é™¤å»
        dept = re.sub(r'(å­¦éƒ¨|å­¦ç§‘|ç ”ç©¶ç§‘|å°‚æ”»|åˆ†é‡|æ•™å®¤)', '', dept)
        dept = re.sub(r'\s+', ' ', dept).strip()
        
        return dept
    
    def extract_research_keywords(self, content: str) -> List[str]:
        """ç ”ç©¶å†…å®¹ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        if not content:
            return []
        
        # ä¸€èˆ¬çš„ãªç ”ç©¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        keyword_patterns = [
            r'([A-Za-z]+ç´°èƒ)',  # XXç´°èƒ
            r'([A-Za-z]+ç™‚æ³•)',  # XXç™‚æ³•
            r'([A-Za-z]+å…ç–«)',  # XXå…ç–«
            r'([A-Za-z]+å­¦)',    # XXå­¦
        ]
        
        keywords = []
        for pattern in keyword_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        return list(set(keywords))
EOF

# ç ”ç©¶å†…å®¹ãƒ‘ãƒ¼ã‚µãƒ¼
cat > scraper/infrastructure/parsers/content_parser.py << 'EOF'
"""
ç ”ç©¶å†…å®¹è§£æãƒ‘ãƒ¼ã‚µãƒ¼
å¤§å­¦ã‚µã‚¤ãƒˆã‹ã‚‰ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º
"""

import re
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup, Tag

from scraper.infrastructure.parsers.base_parser import BaseHtmlParser
from scraper.domain.keyword_analyzer import keyword_analyzer


class UniversityContentParser(BaseHtmlParser):
    """å¤§å­¦ç ”ç©¶å®¤ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è§£æãƒ‘ãƒ¼ã‚µãƒ¼"""
    
    def __init__(self, university_name: str):
        super().__init__()
        self.university_name = university_name
        self._research_lab_indicators = [
            'ç ”ç©¶å®¤', 'ç ”ç©¶é™¢', 'ç ”ç©¶æ‰€', 'ç ”ç©¶ã‚»ãƒ³ã‚¿ãƒ¼',
            'laboratory', 'lab', 'research', 'center'
        ]
        self._professor_indicators = [
            'æ•™æˆ', 'å‡†æ•™æˆ', 'è¬›å¸«', 'åŠ©æ•™',
            'professor', 'prof', 'dr', 'doctor'
        ]
    
    def extract_research_labs(self) -> List[Dict[str, str]]:
        """ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º"""
        if not self._soup:
            return []
        
        labs = []
        
        # æ–¹æ³•1: ç ”ç©¶å®¤ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’æ¢ã™
        lab_list_sections = self._find_lab_list_sections()
        for section in lab_list_sections:
            labs.extend(self._extract_labs_from_section(section))
        
        # æ–¹æ³•2: å€‹åˆ¥ç ”ç©¶å®¤ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        lab_links = self._find_lab_links()
        for link_data in lab_links:
            labs.append(link_data)
        
        # é‡è¤‡é™¤å»ã¨ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª
        cleaned_labs = self._clean_and_validate_labs(labs)
        
        return cleaned_labs
    
    def _find_lab_list_sections(self) -> List[Tag]:
        """ç ”ç©¶å®¤ä¸€è¦§ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢"""
        sections = []
        
        # ç ”ç©¶å®¤ä¸€è¦§ã‚’ç¤ºã™ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œç´¢
        header_patterns = [
            r'ç ”ç©¶å®¤.*ä¸€è¦§', r'ç ”ç©¶.*åˆ†é‡', r'æ•™å“¡.*ç´¹ä»‹',
            r'research.*lab', r'faculty.*member'
        ]
        
        for pattern in header_patterns:
            headers = self._soup.find_all(
                ['h1', 'h2', 'h3', 'h4'], 
                string=re.compile(pattern, re.IGNORECASE)
            )
            
            for header in headers:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã®å¾Œã®è¦ç´ ã‚’å–å¾—
                section = header.find_next(['div', 'section', 'ul', 'table'])
                if section:
                    sections.append(section)
        
        return sections
    
    def _extract_labs_from_section(self, section: Tag) -> List[Dict[str, str]]:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º"""
        labs = []
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼
        if section.name == 'table':
            labs.extend(self._extract_from_table(section))
        
        # ãƒªã‚¹ãƒˆå½¢å¼
        elif section.name == 'ul':
            labs.extend(self._extract_from_list(section))
        
        # divå½¢å¼
        else:
            labs.extend(self._extract_from_div_section(section))
        
        return labs
    
    def _extract_from_table(self, table: Tag) -> List[Dict[str, str]]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º"""
        labs = []
        rows = table.find_all('tr')
        
        for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 2:
                lab_data = self._extract_lab_data_from_cells(cells)
                if lab_data:
                    labs.append(lab_data)
        
        return labs
    
    def _extract_from_list(self, ul_element: Tag) -> List[Dict[str, str]]:
        """ãƒªã‚¹ãƒˆã‹ã‚‰ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º"""
        labs = []
        items = ul_element.find_all('li')
        
        for item in items:
            text = self.extract_text_content(item)
            lab_data = self._parse_lab_text(text)
            
            # ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã¯URLã‚’è¿½åŠ 
            link = item.find('a')
            if link and link.get('href'):
                lab_data['lab_url'] = link['href']
            
            if lab_data and lab_data.get('name'):
                labs.append(lab_data)
        
        return labs
    
    def _extract_from_div_section(self, section: Tag) -> List[Dict[str, str]]:
        """divã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º"""
        labs = []
        
        # ç ”ç©¶å®¤ã‚‰ã—ã„divã‚’æ¤œç´¢
        lab_divs = section.find_all('div', class_=re.compile(r'lab|research|member'))
        
        for div in lab_divs:
            text = self.extract_text_content(div)
            lab_data = self._parse_lab_text(text)
            
            if lab_data and lab_data.get('name'):
                labs.append(lab_data)
        
        return labs
    
    def _extract_lab_data_from_cells(self, cells: List[Tag]) -> Optional[Dict[str, str]]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‚»ãƒ«ã‹ã‚‰ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        if len(cells) < 2:
            return None
        
        # æœ€åˆã®ã‚»ãƒ«ã¯é€šå¸¸ç ”ç©¶å®¤åã¾ãŸã¯æ•™æˆå
        first_cell_text = self.extract_text_content(cells[0])
        second_cell_text = self.extract_text_content(cells[1])
        
        lab_data = {}
        
        # ç ”ç©¶å®¤åã¾ãŸã¯æ•™æˆåã‚’åˆ¤å®š
        if any(indicator in first_cell_text for indicator in self._research_lab_indicators):
            lab_data['name'] = first_cell_text
            lab_data['professor_name'] = second_cell_text
        elif any(indicator in second_cell_text for indicator in self._professor_indicators):
            lab_data['professor_name'] = first_cell_text
            lab_data['name'] = second_cell_text
        else:
            # åˆ¤å®šã§ããªã„å ´åˆã¯åå‰ã¨ã—ã¦æ‰±ã†
            lab_data['name'] = first_cell_text
            lab_data['professor_name'] = second_cell_text
        
        # è¿½åŠ ã‚»ãƒ«ãŒã‚ã‚‹å ´åˆ
        if len(cells) > 2:
            lab_data['research_content'] = self.extract_text_content(cells[2])
        
        return lab_data
    
    def _parse_lab_text(self, text: str) -> Dict[str, str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç ”ç©¶å®¤æƒ…å ±ã‚’è§£æ"""
        lab_data = {
            'name': '',
            'professor_name': '',
            'research_content': text,
            'department': ''
        }
        
        # ç ”ç©¶å®¤åãƒ‘ã‚¿ãƒ¼ãƒ³
        lab_name_patterns = [
            r'([^ã€‚]+ç ”ç©¶å®¤)',
            r'([^ã€‚]+ç ”ç©¶é™¢)',
            r'([^ã€‚]+ç ”ç©¶æ‰€)',
            r'([^ã€‚]+Laboratory)',
            r'([^ã€‚]+Lab)'
        ]
        
        for pattern in lab_name_patterns:
            match = re.search(pattern, text)
            if match:
                lab_data['name'] = match.group(1).strip()
                break
        
        # æ•™æˆåãƒ‘ã‚¿ãƒ¼ãƒ³
        professor_patterns = [
            r'([^\s]+)\s*(æ•™æˆ|å‡†æ•™æˆ|è¬›å¸«|åŠ©æ•™)',
            r'(Prof\.|Dr\.)\s*([^\s]+)',
            r'æ•™æˆ[ï¼š:]\s*([^\s]+)'
        ]
        
        for pattern in professor_patterns:
            match = re.search(pattern, text)
            if match:
                if 'æ•™æˆ' in pattern:
                    lab_data['professor_name'] = match.group(1).strip()
                else:
                    lab_data['professor_name'] = match.group(2).strip()
                break
        
        # ç ”ç©¶å®¤åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ•™æˆåã‹ã‚‰ç”Ÿæˆ
        if not lab_data['name'] and lab_data['professor_name']:
            lab_data['name'] = f"{lab_data['professor_name']}ç ”ç©¶å®¤"
        
        return lab_data
    
    def _find_lab_links(self) -> List[Dict[str, str]]:
        """ç ”ç©¶å®¤ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢"""
        links_data = []
        
        # ç ”ç©¶å®¤ãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        link_patterns = [
            r'ç ”ç©¶å®¤', r'research', r'lab', r'æ•™å“¡'
        ]
        
        for pattern in link_patterns:
            links = self._soup.find_all('a', string=re.compile(pattern, re.IGNORECASE))
            
            for link in links:
                href = link.get('href')
                link_text = self.extract_text_content(link)
                
                if href and self._is_valid_lab_link(href, link_text):
                    lab_data = {
                        'name': link_text,
                        'lab_url': href,
                        'professor_name': '',
                        'research_content': '',
                        'department': ''
                    }
                    links_data.append(lab_data)
        
        return links_data
    
    def _is_valid_lab_link(self, href: str, link_text: str) -> bool:
        """æœ‰åŠ¹ãªç ”ç©¶å®¤ãƒªãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # ç„¡åŠ¹ãªãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        invalid_patterns = [
            r'\.pdf$', r'\.doc$', r'\.ppt$',  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªãƒ³ã‚¯
            r'mailto:', r'tel:',              # ãƒ¡ãƒ¼ãƒ«ãƒ»é›»è©±ãƒªãƒ³ã‚¯
            r'javascript:', r'#'              # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»ã‚¢ãƒ³ã‚«ãƒ¼
        ]
        
        for pattern in invalid_patterns:
            if re.search(pattern, href, re.IGNORECASE):
                return False
        
        # æœ‰åŠ¹ãªãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³
        valid_text_patterns = [
            r'ç ”ç©¶å®¤', r'ç ”ç©¶æ‰€', r'ç ”ç©¶ã‚»ãƒ³ã‚¿ãƒ¼',
            r'laboratory', r'research', r'lab'
        ]
        
        return any(re.search(pattern, link_text, re.IGNORECASE) for pattern in valid_text_patterns)
    
    def _clean_and_validate_labs(self, labs: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼"""
        cleaned_labs = []
        seen_names = set()
        
        for lab in labs:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
            if not lab.get('name') or not lab.get('name').strip():
                continue
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            lab_name = lab['name'].strip()
            if lab_name in seen_names:
                continue
            seen_names.add(lab_name)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_lab = {
                'name': lab_name,
                'professor_name': self.clean_professor_name(lab.get('professor_name', '')),
                'research_content': lab.get('research_content', '').strip(),
                'department': self.clean_department_name(lab.get('department', '')),
                'lab_url': lab.get('lab_url', ''),
                'keywords': lab.get('keywords', '')
            }
            
            # å…ç–«é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            if cleaned_lab['research_content']:
                analysis_result = keyword_analyzer.analyze_content(cleaned_lab['research_content'])
                cleaned_lab['immune_relevance_score'] = analysis_result.immune_relevance_score
                cleaned_lab['research_field'] = analysis_result.field_classification
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±åˆ
                if analysis_result.matched_keywords:
                    existing_keywords = cleaned_lab.get('keywords', '')
                    new_keywords = ', '.join(analysis_result.matched_keywords)
                    cleaned_lab['keywords'] = f"{existing_keywords}, {new_keywords}".strip(', ')
            
            cleaned_labs.append(cleaned_lab)
        
        return cleaned_labs
EOF

echo "âœ… ãƒ‘ãƒ¼ã‚µãƒ¼åŸºç›¤å®Ÿè£…å®Œäº†"

echo ""
echo "ğŸ‰ Infrastructureå±¤å®Ÿè£…å®Œäº†ï¼"
echo ""
echo "ğŸ“‹ å®Ÿè£…ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:"
echo "â”œâ”€â”€ scraper/infrastructure/"
echo "â”‚   â”œâ”€â”€ http/"
echo "â”‚   â”‚   â”œâ”€â”€ rate_limiter.py      # é©å¿œçš„ãƒ¬ãƒ¼ãƒˆåˆ¶é™å™¨"
echo "â”‚   â”‚   â”œâ”€â”€ http_client.py       # å …ç‰¢ãªHTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"
echo "â”‚   â”‚   â””â”€â”€ retry_handler.py     # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•å†è©¦è¡Œ"
echo "â”‚   â””â”€â”€ parsers/"
echo "â”‚       â”œâ”€â”€ base_parser.py       # HTMLè§£æåŸºåº•ã‚¯ãƒ©ã‚¹"
echo "â”‚       â””â”€â”€ content_parser.py    # ç ”ç©¶å†…å®¹è§£æãƒ‘ãƒ¼ã‚µãƒ¼"
echo ""
echo "ğŸ—ï¸ Infrastructureå±¤ã®ç‰¹å¾´:"
echo "â€¢ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼šå¤§å­¦ã‚µãƒ¼ãƒãƒ¼ã¸ã®è² è·åˆ¶å¾¡"
echo "â€¢ å†è©¦è¡Œæ©Ÿæ§‹ï¼šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼è€æ€§"
echo "â€¢ å‹å®‰å…¨æ€§ï¼šå®Œå…¨ãªå‹ãƒ’ãƒ³ãƒˆ"
echo "â€¢ æ‹¡å¼µæ€§ï¼šæ–°ã—ã„ãƒ‘ãƒ¼ã‚µãƒ¼ã®å®¹æ˜“ãªè¿½åŠ "
echo "â€¢ å …ç‰¢æ€§ï¼šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ç›£è¦–"
echo ""
echo "âš¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š"
echo "1. Applicationå±¤ã®å®Ÿè£…"
echo "2. çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"
echo "3. å®Ÿéš›ã®å¤§å­¦ã‚µã‚¤ãƒˆã§ã®ãƒ†ã‚¹ãƒˆ"