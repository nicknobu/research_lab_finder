import asyncio
import aiohttp
from bs4 import BeautifulSoup
import pandas as pd
from dataclasses import dataclass
from typing import List, Optional, Dict
import time
import json
import re
from urllib.parse import urljoin, urlparse
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ResearchLab:
    """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    university_name: str
    department: str
    lab_name: str
    professor_name: str
    research_theme: str
    research_content: str
    research_field: str
    lab_url: str
    prefecture: str
    region: str
    speciality: str = ""
    keywords: List[str] = None

class RealImmuneResearchScraper:
    """å®Ÿéš›ã®å…ç–«ç ”ç©¶å®¤ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Compatible Research Lab Finder Bot) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.8,en;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.delay = 2  # 2ç§’é–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆå¤§å­¦ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=2)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            headers=self.headers,
            connector=connector,
            timeout=timeout
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """Webãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            await asyncio.sleep(self.delay)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            logger.info(f"Fetching: {url}")
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    text = await response.text()
                    logger.info(f"Successfully fetched {url} ({len(text)} chars)")
                    return text
                else:
                    logger.warning(f"Failed to fetch {url}: Status {response.status}")
                    return None
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching {url}")
            return None
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None
    
    def extract_immune_keywords(self, text: str) -> bool:
        """å…ç–«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º"""
        immune_keywords = [
            # åŸºæœ¬çš„ãªå…ç–«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            'å…ç–«', 'immunity', 'immunology', 'immune',
            'Tç´°èƒ', 'Bç´°èƒ', 'NKç´°èƒ', 'ãƒã‚¯ãƒ­ãƒ•ã‚¡ãƒ¼ã‚¸', 'æ¨¹çŠ¶ç´°èƒ',
            
            # åˆ†å­ãƒ»ã‚·ã‚°ãƒŠãƒ«
            'ã‚µã‚¤ãƒˆã‚«ã‚¤ãƒ³', 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ­ãƒ³', 'ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ­ã‚¤ã‚­ãƒ³',
            'æŠ—ä½“', 'antibody', 'antigen', 'æŠ—åŸ', 'IgE', 'IgG',
            
            # ç–¾æ‚£é–¢é€£
            'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼', 'allergy', 'ã‚¢ãƒˆãƒ”ãƒ¼', 'atopic',
            'è‡ªå·±å…ç–«', 'autoimmune', 'autoimmunity',
            'ãƒ¯ã‚¯ãƒãƒ³', 'vaccine', 'äºˆé˜²æ¥ç¨®',
            'ãŒã‚“å…ç–«', 'cancer immunotherapy', 'tumor immunity',
            'ç‚ç—‡', 'inflammation', 'inflammatory',
            
            # ç‰¹æ®Šãªå…ç–«å­¦ç”¨èª
            'Tollæ§˜å—å®¹ä½“', 'TLR', 'RANKL', 'IRF8', 'IRF5',
            'åˆ¶å¾¡æ€§Tç´°èƒ', 'Treg', 'regulatory T cell',
            'å…ç–«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ', 'checkpoint', 'PD-1', 'CTLA-4',
            'ç²˜è†œå…ç–«', 'mucosal immunity', 'è…¸ç®¡å…ç–«',
            'è‡ªç„¶å…ç–«', 'innate immunity', 'ç²å¾—å…ç–«', 'adaptive immunity',
            
            # ç ”ç©¶æ‰‹æ³•
            'single cell', 'ã‚·ãƒ³ã‚°ãƒ«ã‚»ãƒ«', 'ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹',
            'ã‚¨ãƒ³ãƒãƒ³ã‚µãƒ¼', 'enhancer', 'ã‚¨ãƒ”ã‚¸ã‚§ãƒãƒ†ã‚£ã‚¯ã‚¹'
        ]
        
        text_lower = text.lower()
        found_keywords = [kw for kw in immune_keywords if kw.lower() in text_lower]
        
        if found_keywords:
            logger.debug(f"Found immune keywords: {found_keywords}")
            return True
        return False
    
    async def scrape_yokohama_cu(self) -> List[ResearchLab]:
        """æ¨ªæµœå¸‚ç«‹å¤§å­¦ å…ç–«å­¦æ•™å®¤ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        labs = []
        
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸
        main_url = 'https://www-user.yokohama-cu.ac.jp/~immunol/'
        html = await self.fetch_page(main_url)
        
        if not html:
            logger.error("Failed to fetch Yokohama City University main page")
            return labs
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # ç ”ç©¶å†…å®¹ã®æŠ½å‡º
        research_content = self.extract_research_content_ycu(soup)
        
        # ç”°æ‘ç ”ç©¶å®¤ã®æƒ…å ±
        lab = ResearchLab(
            university_name="æ¨ªæµœå¸‚ç«‹å¤§å­¦",
            department="åŒ»å­¦ç ”ç©¶ç§‘",
            lab_name="å…ç–«å­¦æ•™å®¤",
            professor_name="ç”°æ‘æ™ºå½¦",
            research_theme="æ¨¹çŠ¶ç´°èƒåˆ†åŒ–åˆ¶å¾¡æ©Ÿæ§‹",
            research_content=research_content,
            research_field="å…ç–«å­¦",
            lab_url=main_url,
            prefecture="ç¥å¥ˆå·çœŒ",
            region="é–¢æ±",
            speciality="æ¨¹çŠ¶ç´°èƒç ”ç©¶ã€è»¢å†™å› å­IRF8ã€è‡ªå·±å…ç–«ç–¾æ‚£",
            keywords=["æ¨¹çŠ¶ç´°èƒ", "IRF8", "è‡ªå·±å…ç–«ç–¾æ‚£", "ã‚¨ãƒ³ãƒãƒ³ã‚µãƒ¼", "ãƒã‚¤ã‚ªã‚¤ãƒ³ãƒ•ã‚©ãƒãƒ†ã‚£ã‚¯ã‚¹"]
        )
        
        labs.append(lab)
        logger.info(f"Added Yokohama City University lab: {lab.lab_name}")
        
        return labs
    
    def extract_research_content_ycu(self, soup: BeautifulSoup) -> str:
        """æ¨ªæµœå¸‚ç«‹å¤§å­¦ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ç ”ç©¶å†…å®¹ã‚’æŠ½å‡º"""
        # ç ”ç©¶å†…å®¹ã®å€™è£œè¦ç´ ã‚’æ¢ã™
        content_selectors = [
            'div.research-content',
            'div.about-research',
            'div.main-content',
            'div.content',
            'main',
            'article'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                text = elements[0].get_text(strip=True)
                if len(text) > 100:
                    return text[:1000]  # æœ€åˆã®1000æ–‡å­—
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…ç–«é–¢é€£éƒ¨åˆ†ã‚’æŠ½å‡º
        all_text = soup.get_text()
        if self.extract_immune_keywords(all_text):
            # å…ç–«é–¢é€£ã®æ®µè½ã‚’æŠ½å‡º
            paragraphs = soup.find_all('p')
            immune_paragraphs = []
            
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 50 and self.extract_immune_keywords(text):
                    immune_paragraphs.append(text)
            
            if immune_paragraphs:
                return ' '.join(immune_paragraphs[:3])  # æœ€åˆã®3æ®µè½
        
        return "æ¨¹çŠ¶ç´°èƒã®åˆ†åŒ–åˆ¶å¾¡æ©Ÿæ§‹ã¨è‡ªå·±å…ç–«ç–¾æ‚£ã®ç—…æ…‹è§£æ˜ã«é–¢ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚è»¢å†™å› å­IRF8ã«ã‚ˆã‚‹éºä¼å­ç™ºç¾åˆ¶å¾¡æ©Ÿæ§‹ã®è§£æã‚’é€šã˜ã¦ã€å…ç–«ç³»ã®ç†è§£ã‚’æ·±ã‚ã€æ–°ã—ã„æ²»ç™‚æ³•ã®é–‹ç™ºã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚"
    
    async def scrape_tokyo_science_university(self) -> List[ResearchLab]:
        """æ±äº¬ç†ç§‘å¤§å­¦ã®å…ç–«å­¦ç ”ç©¶å®¤ã‚’å–å¾—"""
        labs = []
        
        # èª¿æŸ»æ¸ˆã¿ã®ç ”ç©¶å®¤æƒ…å ±ã‚’åŸºã«è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        research_labs_data = [
            {
                'professor': 'è¥¿å±±åƒæ˜¥',
                'department': 'å…ˆé€²å·¥å­¦éƒ¨ ç”Ÿå‘½ã‚·ã‚¹ãƒ†ãƒ å·¥å­¦ç§‘',
                'theme': 'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ã‚„è‡ªå·±å…ç–«ç–¾æ‚£ã®ç™ºç—‡æ©Ÿåºè§£æ˜',
                'content': 'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼ã‚„è‡ªå·±å…ç–«ç–¾æ‚£ã®ç™ºç—‡æ©Ÿåºè§£æ˜ã€å¹¹ç´°èƒã‹ã‚‰å…ç–«ç³»ç´°èƒåˆ†åŒ–ã«ãŠã‘ã‚‹éºä¼å­ç™ºç¾åˆ¶å¾¡æ©Ÿæ§‹ã®è§£æ˜ã€é£Ÿå“ã‚„è…¸å†…ç´°èŒä»£è¬å‰¯ç”£ç‰©ã«ã‚ˆã‚‹å…ç–«å¿œç­”èª¿ç¯€ã«é–¢ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚åˆ†å­ç”Ÿç‰©å­¦ã€ã‚²ãƒãƒ åŒ»ç§‘å­¦ã€å¿œç”¨ç”Ÿå‘½å·¥å­¦ã®æ‰‹æ³•ã‚’ç”¨ã„ã¦ã€å…ç–«ç³»ã®åŸºæœ¬çš„ãªä»•çµ„ã¿ã‹ã‚‰ç–¾æ‚£ã®æ²»ç™‚æ³•é–‹ç™ºã¾ã§å¹…åºƒã„ç ”ç©¶ã‚’å±•é–‹ã—ã¦ã„ã¾ã™ã€‚',
                'url': 'https://www.tus.ac.jp/academics/faculty/industrialscience_technology/biological/',
                'speciality': 'ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼å­¦ã€è‡ªå·±å…ç–«ç–¾æ‚£ã€å¹¹ç´°èƒå…ç–«å­¦',
                'keywords': ['ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼', 'è‡ªå·±å…ç–«ç–¾æ‚£', 'å¹¹ç´°èƒ', 'éºä¼å­ç™ºç¾åˆ¶å¾¡', 'è…¸å†…ç´°èŒ']
            },
            {
                'professor': 'ä¸Šç¾½æ‚Ÿå²',
                'department': 'ç”Ÿå‘½ç§‘å­¦ç ”ç©¶ç§‘',
                'theme': 'ç‚ç—‡ãƒ»å…ç–«å­¦',
                'content': 'ç‚ç—‡æ€§ç–¾æ‚£ã®åˆ†å­ãƒ»ç´°èƒåŸºç›¤ã®è§£æ˜ã€ãŒã‚“å…ç–«ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãŠã‚ˆã³æ–°è¦è¤‡åˆãŒã‚“å…ç–«ç™‚æ³•ã®é–‹ç™ºã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚çµ„ç¹”ã«ç—…åŸä½“ã‚„ç”Ÿä½“ç•°ç‰©ãªã©ã®ä¾µè¥²ãŒèµ·ããŸéš›ã®ç‚ç—‡ãƒ»å…ç–«åå¿œã®éç¨‹ã‚’åˆ†å­ã€ç´°èƒã€çµ„ç¹”ã€å€‹ä½“ãƒ¬ãƒ™ãƒ«ã§è§£æ˜ã—ã€ç¾åœ¨æ²»ç™‚æ³•ã®ãªã„ç‚ç—‡ãƒ»å…ç–«é›£ç—…ã«å¯¾ã™ã‚‹æ²»ç™‚æ³•ã®é–‹ç™ºã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚',
                'url': 'https://www.tus.ac.jp/academics/graduate_school/biologicalsciences/biologicalsciences/',
                'speciality': 'ç‚ç—‡å­¦ã€ãŒã‚“å…ç–«å­¦ã€å…ç–«ç™‚æ³•',
                'keywords': ['ç‚ç—‡', 'ãŒã‚“å…ç–«', 'å…ç–«ç™‚æ³•', 'ç—…åŸä½“', 'çµ„ç¹”ä¿®å¾©']
            },
            {
                'professor': 'ä¹…ä¿å…äºº',
                'department': 'ç”Ÿå‘½ç§‘å­¦ç ”ç©¶ç§‘',
                'theme': 'åˆ†å­ç—…æ…‹å­¦ãƒ»å…ç–«å­¦ãƒ»ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼å­¦',
                'content': 'åˆ¶å¾¡Tç´°èƒã«ã‚ˆã‚‹å…ç–«å¿œç­”ã®æ©Ÿæ§‹ã€ãƒ˜ãƒ«ãƒ‘ãƒ¼Tç´°èƒï¼ˆTh1/Th2/Th17/TFHï¼‰ã®åˆ†åŒ–åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€ã‚µã‚¤ãƒˆã‚«ã‚¤ãƒ³ã‚·ã‚°ãƒŠãƒ«ä¼é”åˆ†å­ã®è§£æã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚ç—…æ‚£ãƒ¢ãƒ‡ãƒ«ãƒã‚¦ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã€Tç´°èƒã«ã‚ˆã‚‹æŠ—ä½“ç”£ç”Ÿèª˜å°ã®åˆ†å­ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è§£æ˜ã€éºä¼å­ãƒãƒƒã‚¯ã‚¢ã‚¦ãƒˆãƒã‚¦ã‚¹ãƒ»ãƒˆãƒ©ãƒ³ã‚¹ã‚¸ã‚§ãƒ‹ãƒƒã‚¯ãƒã‚¦ã‚¹ã®ä½œæˆã‚’é€šã˜ã¦ã€å…ç–«å¿œç­”åˆ¶å¾¡ã®åŸºæœ¬åŸç†ã‚’æ˜ã‚‰ã‹ã«ã—ã¦ã„ã¾ã™ã€‚',
                'url': 'https://www.tus.ac.jp/academics/graduate_school/biologicalsciences/biologicalsciences/',
                'speciality': 'åˆ¶å¾¡Tç´°èƒã€ãƒ˜ãƒ«ãƒ‘ãƒ¼Tç´°èƒã€ã‚µã‚¤ãƒˆã‚«ã‚¤ãƒ³',
                'keywords': ['åˆ¶å¾¡Tç´°èƒ', 'ãƒ˜ãƒ«ãƒ‘ãƒ¼Tç´°èƒ', 'ã‚µã‚¤ãƒˆã‚«ã‚¤ãƒ³', 'Th1', 'Th2', 'Th17']
            },
            {
                'professor': 'æ–°ç”°å‰›',
                'department': 'ç”Ÿå‘½åŒ»ç§‘å­¦ç ”ç©¶æ‰€',
                'theme': 'åˆ†å­ç—…æ…‹å­¦',
                'content': 'éª¨å…ç–«å­¦ã®å°‚é–€å®¶ã¨ã—ã¦ã€å…ç–«ç³»ã¨éª¨ä»£è¬ã®ç›¸äº’ä½œç”¨ã«é–¢ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚RANKL ã‚„éª¨èŠ½ç´°èƒã®æ©Ÿèƒ½åˆ¶å¾¡ã€é–¢ç¯€ç‚ã«ãŠã‘ã‚‹éª¨ç ´å£Šæ©Ÿæ§‹ã®è§£æ˜ã‚’é€šã˜ã¦ã€éª¨ç²—é¬†ç—‡ã‚„é–¢ç¯€ãƒªã‚¦ãƒãƒãªã©ã®ç–¾æ‚£ã®æ–°ã—ã„æ²»ç™‚æ³•é–‹ç™ºã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚æ±äº¬å¤§å­¦é«˜æŸ³ç ”ç©¶å®¤ã§ã®ç ”ç©¶æˆæœã‚’åŸºã«ã€ã‚ˆã‚Šå®Ÿç”¨çš„ãªæ²»ç™‚æ³•ã®é–‹ç™ºã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚',
                'url': 'https://www.ribs.tus.ac.jp/',
                'speciality': 'éª¨å…ç–«å­¦ã€RANKLã€é–¢ç¯€ç‚',
                'keywords': ['éª¨å…ç–«å­¦', 'RANKL', 'é–¢ç¯€ç‚', 'éª¨ç ´å£Š', 'éª¨èŠ½ç´°èƒ']
            }
        ]
        
        # å„ç ”ç©¶å®¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        for lab_data in research_labs_data:
            lab = ResearchLab(
                university_name="æ±äº¬ç†ç§‘å¤§å­¦",
                department=lab_data['department'],
                lab_name=f"{lab_data['professor']}ç ”ç©¶å®¤",
                professor_name=lab_data['professor'],
                research_theme=lab_data['theme'],
                research_content=lab_data['content'],
                research_field="å…ç–«å­¦",
                lab_url=lab_data['url'],
                prefecture="æ±äº¬éƒ½",
                region="é–¢æ±",
                speciality=lab_data['speciality'],
                keywords=lab_data['keywords']
            )
            
            labs.append(lab)
            logger.info(f"Added Tokyo University of Science lab: {lab.lab_name}")
        
        return labs
    
    async def scrape_additional_labs(self) -> List[ResearchLab]:
        """ãã®ä»–ã®è‘—åãªå…ç–«å­¦ç ”ç©¶å®¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        labs = []
        
        # ç­‘æ³¢å¤§å­¦ï¼ˆæ—¢ã«èª¿æŸ»æ¸ˆã¿ï¼‰
        tsukuba_lab = ResearchLab(
            university_name="ç­‘æ³¢å¤§å­¦",
            department="åŒ»å­¦åŒ»ç™‚ç³»",
            lab_name="å…ç–«å­¦ç ”ç©¶å®¤",
            professor_name="æ¸‹è°·å½°",
            research_theme="NKç´°èƒã®æ©Ÿèƒ½åˆ¶å¾¡",
            research_content="NKç´°èƒã‚„ãã®ä»–ã®è‡ªç„¶å…ç–«ç´°èƒã®æ©Ÿèƒ½è§£æã€ã‚¦ã‚¤ãƒ«ã‚¹æ„ŸæŸ“ã«å¯¾ã™ã‚‹å…ç–«å¿œç­”ã€ã‚¢ãƒ¬ãƒ«ã‚®ãƒ¼åå¿œã®åˆ¶å¾¡æ©Ÿæ§‹ã«é–¢ã™ã‚‹ç ”ç©¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚CD300ãƒ•ã‚¡ãƒŸãƒªãƒ¼åˆ†å­ã®æ©Ÿèƒ½è§£æã‚„ã€å…ç–«å—å®¹ä½“ã®åˆ†å­æ©Ÿæ§‹è§£æ˜ã‚’é€šã˜ã¦ã€å…ç–«ç³»ã®ç†è§£ã‚’æ·±ã‚ã¦ã„ã¾ã™ã€‚",
            research_field="å…ç–«å­¦",
            lab_url="http://immuno-tsukuba.com/",
            prefecture="èŒ¨åŸçœŒ",
            region="é–¢æ±",
            speciality="NKç´°èƒã€è‡ªç„¶å…ç–«ã€ã‚¦ã‚¤ãƒ«ã‚¹å…ç–«",
            keywords=["NKç´°èƒ", "è‡ªç„¶å…ç–«", "ã‚¦ã‚¤ãƒ«ã‚¹å…ç–«", "CD300", "å…ç–«å—å®¹ä½“"]
        )
        labs.append(tsukuba_lab)
        
        return labs
    
    async def collect_all_labs(self) -> List[ResearchLab]:
        """ã™ã¹ã¦ã®ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        all_labs = []
        
        try:
            # æ¨ªæµœå¸‚ç«‹å¤§å­¦
            logger.info("=== æ¨ªæµœå¸‚ç«‹å¤§å­¦ å…ç–«å­¦æ•™å®¤ ===")
            ycu_labs = await self.scrape_yokohama_cu()
            all_labs.extend(ycu_labs)
            
            # æ±äº¬ç†ç§‘å¤§å­¦
            logger.info("=== æ±äº¬ç†ç§‘å¤§å­¦ å…ç–«å­¦ç ”ç©¶å®¤ç¾¤ ===")
            tus_labs = await self.scrape_tokyo_science_university()
            all_labs.extend(tus_labs)
            
            # ãã®ä»–ã®ç ”ç©¶å®¤
            logger.info("=== ãã®ä»–ã®å…ç–«å­¦ç ”ç©¶å®¤ ===")
            other_labs = await self.scrape_additional_labs()
            all_labs.extend(other_labs)
            
        except Exception as e:
            logger.error(f"Error during collection: {e}")
        
        return all_labs

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¬ å®Ÿéš›ã®å…ç–«ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    async with RealImmuneResearchScraper() as scraper:
        # å…¨ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        all_labs = await scraper.collect_all_labs()
        
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(all_labs)} ç ”ç©¶å®¤")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        df = pd.DataFrame([
            {
                'university_name': lab.university_name,
                'department': lab.department,
                'lab_name': lab.lab_name,
                'professor_name': lab.professor_name,
                'research_theme': lab.research_theme,
                'research_content': lab.research_content,
                'research_field': lab.research_field,
                'lab_url': lab.lab_url,
                'prefecture': lab.prefecture,
                'region': lab.region,
                'speciality': lab.speciality,
                'keywords': ','.join(lab.keywords) if lab.keywords else ''
            }
            for lab in all_labs
        ])
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        filename = f'real_immune_research_labs_{timestamp}.csv'
        df.to_csv(filename, index=False, encoding='utf-8')
        print(f"ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ '{filename}' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        print("\nğŸ“ˆ åé›†ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        print(f"- ç ”ç©¶å®¤æ•°: {len(df)}")
        print(f"- å¤§å­¦æ•°: {df['university_name'].nunique()}")
        print(f"- åœ°åŸŸåˆ†å¸ƒ: {df['region'].value_counts().to_dict()}")
        print(f"- ç ”ç©¶åˆ†é‡: {df['research_field'].value_counts().to_dict()}")
        
        # å„ç ”ç©¶å®¤ã®è©³ç´°è¡¨ç¤º
        print("\nğŸ”¬ åé›†ã—ãŸç ”ç©¶å®¤ä¸€è¦§:")
        for i, lab in enumerate(all_labs, 1):
            print(f"\n{i}. {lab.lab_name} ({lab.university_name})")
            print(f"   æ•™æˆ: {lab.professor_name}")
            print(f"   å°‚é–€: {lab.speciality}")
            print(f"   ç ”ç©¶ãƒ†ãƒ¼ãƒ: {lab.research_theme}")
            print(f"   æ‰€åœ¨åœ°: {lab.prefecture} ({lab.region}åœ°åŸŸ)")
    
    return df

if __name__ == "__main__":
    # å®Ÿè¡Œ
    result_df = asyncio.run(main())
