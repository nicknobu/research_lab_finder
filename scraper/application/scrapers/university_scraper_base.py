"""
å¤§å­¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹
å…±é€šã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã‚’æä¾›
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Any
import asyncio
import logging
from urllib.parse import urljoin, urlparse

from scraper.config.interfaces import (
    UniversityScraperInterface, ResearchLabData, AdmissionData,
    ScrapingError, DataValidationError
)
from scraper.domain.university import University
from scraper.domain.research_lab import create_research_lab_from_data
from scraper.infrastructure.http.http_client import ResearchLabHttpClient
from scraper.infrastructure.parsers.content_parser import UniversityContentParser
from scraper.config.settings import scraping_settings

logger = logging.getLogger(__name__)


class UniversityScraperBase(UniversityScraperInterface):
    """å¤§å­¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, university: University):
        self.university = university
        self.base_url = university.info.website_url or ""
        self.http_client: Optional[ResearchLabHttpClient] = None
        self.parser: Optional[UniversityContentParser] = None
        self._scraped_labs: List[ResearchLabData] = []
        self._scraped_admissions: List[AdmissionData] = []
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        self.http_client = ResearchLabHttpClient()
        await self.http_client.__aenter__()
        self.parser = UniversityContentParser(self.university.name)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        if self.http_client:
            await self.http_client.__aexit__(exc_type, exc_val, exc_tb)
    
    async def scrape_research_labs(self) -> List[ResearchLabData]:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆåŸºæœ¬å®Ÿè£…ï¼‰"""
        try:
            logger.info(f"ğŸ” {self.university.name} ã®ç ”ç©¶å®¤ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
            
            # å¯¾è±¡URLãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
            target_urls = await self._discover_target_urls()
            
            # å„URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            for url in target_urls:
                try:
                    labs = await self._scrape_single_page(url)
                    self._scraped_labs.extend(labs)
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                    logger.info(f"âœ… {url}: {len(labs)}ä»¶ã®ç ”ç©¶å®¤ã‚’ç™ºè¦‹")
                    
                except Exception as e:
                    logger.error(f"âŒ {url} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
                    continue
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            validated_labs = await self._validate_and_enrich_labs(self._scraped_labs)
            
            logger.info(f"ğŸ‰ {self.university.name}: åˆè¨ˆ{len(validated_labs)}ä»¶ã®ç ”ç©¶å®¤ã‚’åé›†")
            return validated_labs
            
        except Exception as e:
            logger.error(f"âŒ {self.university.name} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
            raise ScrapingError(f"Failed to scrape {self.university.name}: {e}")
    
    async def scrape_admission_info(self) -> List[AdmissionData]:
        """ç·åˆå‹é¸æŠœæƒ…å ±ã®å–å¾—ï¼ˆåŸºæœ¬å®Ÿè£…ï¼‰"""
        try:
            logger.info(f"ğŸ“‹ {self.university.name} ã®ç·åˆå‹é¸æŠœæƒ…å ±åé›†é–‹å§‹")
            
            # å…¥è©¦æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢
            admission_urls = await self._discover_admission_urls()
            
            for url in admission_urls:
                try:
                    admissions = await self._scrape_admission_page(url)
                    self._scraped_admissions.extend(admissions)
                    
                except Exception as e:
                    logger.error(f"âŒ ç·åˆå‹é¸æŠœæƒ…å ±åé›†å¤±æ•— {url}: {e}")
                    continue
            
            logger.info(f"ğŸ“Š {self.university.name}: {len(self._scraped_admissions)}ä»¶ã®é¸æŠœæƒ…å ±ã‚’åé›†")
            return self._scraped_admissions
            
        except Exception as e:
            logger.error(f"âŒ {self.university.name} ã®ç·åˆå‹é¸æŠœæƒ…å ±åé›†å¤±æ•—: {e}")
            return []
    
    def validate_data(self, data: ResearchLabData) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
            required_fields = ['name', 'professor_name', 'research_content']
            for field in required_fields:
                value = getattr(data, field, None)
                if not value or (isinstance(value, str) and len(value.strip()) < 3):
                    return False
            
            # ç ”ç©¶å†…å®¹ã®é•·ã•ãƒã‚§ãƒƒã‚¯
            if len(data.research_content) < scraping_settings.min_content_length:
                return False
            
            # å…ç–«é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œã§å®Ÿè£…ï¼‰
            # if data.immune_relevance_score and data.immune_relevance_score < 0.3:
            #     return False
            
            return True
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def _discover_target_urls(self) -> List[str]:
        """å¯¾è±¡URLã‚’ç™ºè¦‹ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        urls = []
        
        if not self.base_url:
            logger.warning(f"{self.university.name}: ãƒ™ãƒ¼ã‚¹URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return urls
        
        # å­¦éƒ¨ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        faculty_urls = [
            f"{self.base_url}/medicine/",
            f"{self.base_url}/medical/",
            f"{self.base_url}/graduate/",
            f"{self.base_url}/research/",
            f"{self.base_url}/faculty/"
        ]
        
        urls.extend(faculty_urls)
        return urls
    
    async def _discover_admission_urls(self) -> List[str]:
        """ç·åˆå‹é¸æŠœURLã‚’ç™ºè¦‹"""
        urls = []
        
        if not self.base_url:
            return urls
        
        # å…¥è©¦æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        admission_urls = [
            f"{self.base_url}/admissions/",
            f"{self.base_url}/entrance/",
            f"{self.base_url}/exam/",
            f"{self.base_url}/nyushi/",
            f"{self.base_url}/ao/"
        ]
        
        urls.extend(admission_urls)
        return urls
    
    async def _scrape_single_page(self, url: str) -> List[ResearchLabData]:
        """å˜ä¸€ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            # HTMLã‚’å–å¾—
            html_content = await self.http_client.get_text(url)
            
            # ãƒ‘ãƒ¼ã‚¹
            self.parser.parse_html(html_content)
            
            # ç ”ç©¶å®¤æƒ…å ±ã‚’æŠ½å‡º
            raw_labs = self.parser.extract_research_labs()
            
            # ResearchLabDataã«å¤‰æ›
            lab_data_list = []
            for raw_lab in raw_labs:
                try:
                    lab_data = self._convert_to_research_lab_data(raw_lab, url)
                    if self.validate_data(lab_data):
                        lab_data_list.append(lab_data)
                except Exception as e:
                    logger.warning(f"ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿å¤‰æ›å¤±æ•—: {e}")
                    continue
            
            return lab_data_list
            
        except Exception as e:
            logger.error(f"ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— {url}: {e}")
            return []
    
    async def _scrape_admission_page(self, url: str) -> List[AdmissionData]:
        """ç·åˆå‹é¸æŠœãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        try:
            html_content = await self.http_client.get_text(url)
            # ç·åˆå‹é¸æŠœæƒ…å ±ã®æŠ½å‡ºï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…
            admission_data = AdmissionData(
                university_id=self.university.info.id,
                faculty="åŒ»å­¦éƒ¨",  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯å‹•çš„ã«æŠ½å‡º
                department=None,
                is_available=True,
                quota="è‹¥å¹²å",
                info_url=url
            )
            
            return [admission_data]
            
        except Exception as e:
            logger.error(f"ç·åˆå‹é¸æŠœãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•— {url}: {e}")
            return []
    
    def _convert_to_research_lab_data(self, raw_lab: Dict[str, str], source_url: str) -> ResearchLabData:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ResearchLabDataã«å¤‰æ›"""
        return ResearchLabData(
            name=raw_lab.get('name', '').strip(),
            professor_name=raw_lab.get('professor_name', '').strip(),
            department=raw_lab.get('department', '').strip(),
            faculty=self._determine_faculty_from_url(source_url),
            research_content=raw_lab.get('research_content', '').strip(),
            university_id=self.university.info.id,
            lab_url=self._resolve_lab_url(raw_lab.get('lab_url', ''), source_url),
            keywords=raw_lab.get('keywords', ''),
            immune_relevance_score=raw_lab.get('immune_relevance_score')
        )
    
    def _determine_faculty_from_url(self, url: str) -> str:
        """URLã‹ã‚‰å­¦éƒ¨ã‚’æ¨å®š"""
        url_lower = url.lower()
        
        if any(keyword in url_lower for keyword in ['medical', 'medicine', 'åŒ»å­¦']):
            return 'medicine'
        elif any(keyword in url_lower for keyword in ['agriculture', 'è¾²å­¦', 'è¾²æ¥­']):
            return 'agriculture'
        elif any(keyword in url_lower for keyword in ['veterinary', 'ç£åŒ»']):
            return 'veterinary'
        elif any(keyword in url_lower for keyword in ['science', 'ç†å­¦']):
            return 'science'
        else:
            return 'general'
    
    def _resolve_lab_url(self, lab_url: str, base_url: str) -> str:
        """ç ”ç©¶å®¤URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
        if not lab_url:
            return ""
        
        if lab_url.startswith('http'):
            return lab_url
        
        return urljoin(base_url, lab_url)
    
    async def _validate_and_enrich_labs(self, labs: List[ResearchLabData]) -> List[ResearchLabData]:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ã¨å¼·åŒ–"""
        validated_labs = []
        
        for lab in labs:
            try:
                # åŸºæœ¬ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                if not self.validate_data(lab):
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿å¼·åŒ–ï¼ˆå…ç–«é–¢é€£åº¦è¨ˆç®—ãªã©ï¼‰
                enriched_lab = await self._enrich_lab_data(lab)
                validated_labs.append(enriched_lab)
                
            except Exception as e:
                logger.warning(f"ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å¤±æ•—: {e}")
                continue
        
        return validated_labs
    
    async def _enrich_lab_data(self, lab: ResearchLabData) -> ResearchLabData:
        """ç ”ç©¶å®¤ãƒ‡ãƒ¼ã‚¿ã®å¼·åŒ–"""
        # å…ç–«é–¢é€£åº¦åˆ†æ
        from scraper.domain.keyword_analyzer import keyword_analyzer
        
        if lab.research_content:
            analysis_result = keyword_analyzer.analyze_content(lab.research_content)
            lab.immune_relevance_score = analysis_result.immune_relevance_score
            lab.research_field = analysis_result.field_classification
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±åˆ
            if analysis_result.matched_keywords:
                existing_keywords = lab.keywords or ''
                new_keywords = ', '.join(analysis_result.matched_keywords)
                lab.keywords = f"{existing_keywords}, {new_keywords}".strip(', ')
        
        return lab
